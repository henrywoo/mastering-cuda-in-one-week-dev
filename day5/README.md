# Day 5: æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer - ç°ä»£NLPçš„CUDAå®ç°

## æ¦‚è¿°
ä»Šå¤©æˆ‘ä»¬å°†å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶(Attention)å’ŒTransformerçš„CUDAå®ç°ã€‚è¿™æ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€ï¼ŒåŒ…æ‹¬BERTã€GPTç­‰æ¨¡å‹çš„æ ¸å¿ƒç†å¿µã€‚æˆ‘ä»¬å°†æ·±å…¥ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹å’Œå„ç§ä¼˜åŒ–æŠ€å·§ã€‚

## å­¦ä¹ ç›®æ ‡
- ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬åŸç†å’Œæ•°å­¦å…¬å¼
- æŒæ¡è‡ªæ³¨æ„åŠ›(Self-Attention)çš„CUDAå®ç°
- å­¦ä¼šå®ç°å¤šå¤´æ³¨æ„åŠ›(Multi-Head Attention)
- ç†è§£ä½ç½®ç¼–ç å’Œå±‚å½’ä¸€åŒ–çš„å®ç°
- æŒæ¡Transformeræ¶æ„çš„å®Œæ•´å®ç°

## æ³¨æ„åŠ›æœºåˆ¶åŸºç¡€

### 1. æ•°å­¦å®šä¹‰
æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ˜¯è®¡ç®—æŸ¥è¯¢(Query)ã€é”®(Key)ã€å€¼(Value)ä¹‹é—´çš„å…³ç³»ï¼š
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

å…¶ä¸­ï¼š
- Q: æŸ¥è¯¢çŸ©é˜µ (batch_size Ã— seq_len Ã— d_k)
- K: é”®çŸ©é˜µ (batch_size Ã— seq_len Ã— d_k)
- V: å€¼çŸ©é˜µ (batch_size Ã— seq_len Ã— d_v)
- d_k: é”®çš„ç»´åº¦

### 2. è®¡ç®—æ­¥éª¤
1. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**: S = QK^T
2. **ç¼©æ”¾**: S' = S / âˆšd_k
3. **åº”ç”¨softmax**: A = softmax(S')
4. **åŠ æƒæ±‚å’Œ**: Output = A Ã— V

## è‡ªæ³¨æ„åŠ›å®ç°

### 1. åŸºç¡€è‡ªæ³¨æ„åŠ›Kernel
```cpp
__global__ void selfAttentionKernel(float *Q, float *K, float *V, float *output,
                                   int batchSize, int seqLen, int d_k, int d_v) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int threadIdx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || headIdx >= d_k) {
        return;
    }
    
    // è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    float attention_scores[seqLen];
    float max_score = -INFINITY;
    
    // ç¬¬ä¸€æ­¥ï¼šè®¡ç®—QK^Tå¹¶æ‰¾åˆ°æœ€å¤§å€¼
    for (int j = 0; j < seqLen; j++) {
        float score = 0.0f;
        for (int k = 0; k < d_k; k++) {
            score += Q[batchIdx * seqLen * d_k + seqIdx * d_k + k] * 
                     K[batchIdx * seqLen * d_k + j * d_k + k];
        }
        attention_scores[j] = score / sqrtf(d_k);
        max_score = max(max_score, attention_scores[j]);
    }
    
    // ç¬¬äºŒæ­¥ï¼šåº”ç”¨softmax
    float sum_exp = 0.0f;
    for (int j = 0; j < seqLen; j++) {
        attention_scores[j] = expf(attention_scores[j] - max_score);
        sum_exp += attention_scores[j];
    }
    
    for (int j = 0; j < seqLen; j++) {
        attention_scores[j] /= sum_exp;
    }
    
    // ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—åŠ æƒè¾“å‡º
    for (int v = 0; v < d_v; v++) {
        float weighted_sum = 0.0f;
        for (int j = 0; j < seqLen; j++) {
            weighted_sum += attention_scores[j] * 
                           V[batchIdx * seqLen * d_v + j * d_v + v];
        }
        output[batchIdx * seqLen * d_v + seqIdx * d_v + v] = weighted_sum;
    }
}
```

### 2. ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å…±äº«å†…å­˜
```cpp
__global__ void selfAttentionSharedKernel(float *Q, float *K, float *V, float *output,
                                         int batchSize, int seqLen, int d_k, int d_v) {
    __shared__ float s_attention_scores[MAX_SEQ_LEN];
    __shared__ float s_temp_values[MAX_SEQ_LEN];
    
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int tx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || headIdx >= d_k) {
        return;
    }
    
    // åä½œè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    for (int j = tx; j < seqLen; j += blockDim.x) {
        float score = 0.0f;
        for (int k = 0; k < d_k; k++) {
            score += Q[batchIdx * seqLen * d_k + seqIdx * d_k + k] * 
                     K[batchIdx * seqLen * d_k + j * d_k + k];
        }
        s_attention_scores[j] = score / sqrtf(d_k);
    }
    __syncthreads();
    
    // åä½œè®¡ç®—softmax
    float max_score = -INFINITY;
    for (int j = tx; j < seqLen; j += blockDim.x) {
        max_score = max(max_score, s_attention_scores[j]);
    }
    
    // è§„çº¦æ±‚æœ€å¤§å€¼
    for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if (tx < stride) {
            s_temp_values[tx] = max(s_temp_values[tx], s_temp_values[tx + stride]);
        }
        __syncthreads();
    }
    
    if (tx == 0) {
        max_score = s_temp_values[0];
    }
    __syncthreads();
    
    // åº”ç”¨softmax
    for (int j = tx; j < seqLen; j += blockDim.x) {
        s_attention_scores[j] = expf(s_attention_scores[j] - max_score);
    }
    __syncthreads();
    
    // è®¡ç®—è¾“å‡º
    for (int v = tx; v < d_v; v += blockDim.x) {
        float weighted_sum = 0.0f;
        for (int j = 0; j < seqLen; j++) {
            weighted_sum += s_attention_scores[j] * 
                           V[batchIdx * seqLen * d_v + j * d_v + v];
        }
        output[batchIdx * seqLen * d_v + seqIdx * d_v + v] = weighted_sum;
    }
}
```

## å¤šå¤´æ³¨æ„åŠ›å®ç°

### 1. å¤šå¤´æ³¨æ„åŠ›ç»“æ„
```cpp
struct MultiHeadAttention {
    int numHeads;
    int d_model;
    int d_k;
    int d_v;
    
    float *W_q, *W_k, *W_v;  // çº¿æ€§å˜æ¢æƒé‡
    float *W_o;               // è¾“å‡ºæŠ•å½±æƒé‡
    float *b_q, *b_k, *b_v, *b_o;  // åç½®é¡¹
};

__global__ void multiHeadAttentionKernel(MultiHeadAttention *mha,
                                        float *input, float *output,
                                        int batchSize, int seqLen) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int threadIdx = threadIdx.x;
    
    // è®¡ç®—å½“å‰å¤´éƒ¨çš„åç§»
    int headOffset = headIdx * mha->d_k;
    
    // çº¿æ€§å˜æ¢ï¼šQ, K, V
    float Q[MAX_D_K], K[MAX_D_K], V[MAX_D_V];
    
    // è®¡ç®—Q
    for (int k = 0; k < mha->d_k; k++) {
        Q[k] = 0.0f;
        for (int i = 0; i < mha->d_model; i++) {
            Q[k] += input[batchIdx * seqLen * mha->d_model + seqIdx * mha->d_model + i] * 
                     mha->W_q[i * mha->d_k + k];
        }
        Q[k] += mha->b_q[headOffset + k];
    }
    
    // ç±»ä¼¼åœ°è®¡ç®—Kå’ŒV
    // ...
    
    // è®¡ç®—æ³¨æ„åŠ›
    // ...
    
    // è¾“å‡ºæŠ•å½±
    // ...
}
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–
```cpp
// ä½¿ç”¨æ‰¹å¤„ç†çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
void multiHeadAttentionOptimized(MultiHeadAttention *mha,
                                float *input, float *output,
                                int batchSize, int seqLen) {
    // é‡å¡‘è¾“å…¥ä¸º(batch_size * seq_len, d_model)
    // ä½¿ç”¨cuBLASè¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•
    
    // è®¡ç®—Q, K, V
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                 d_k, batchSize * seqLen, d_model,
                 &alpha, mha->W_q, d_k,
                 input, d_model,
                 &beta, Q, d_k);
    
    // ç±»ä¼¼åœ°è®¡ç®—Kå’ŒV
    // ...
    
    // è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    // ...
}
```

## ä½ç½®ç¼–ç å®ç°

### 1. æ­£å¼¦ä½ç½®ç¼–ç 
```cpp
__global__ void positionalEncodingKernel(float *pos_enc, int seqLen, int d_model) {
    int pos = blockIdx.x * blockDim.x + threadIdx.x;
    int dim = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (pos < seqLen && dim < d_model) {
        if (dim % 2 == 0) {
            pos_enc[pos * d_model + dim] = sinf(pos / powf(10000.0f, dim / d_model));
        } else {
            pos_enc[pos * d_model + dim] = cosf(pos / powf(10000.0f, (dim-1) / d_model));
        }
    }
}
```

### 2. ç›¸å¯¹ä½ç½®ç¼–ç 
```cpp
__global__ void relativePositionalEncodingKernel(float *rel_pos_enc,
                                                int seqLen, int d_k) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (i < seqLen && j < seqLen) {
        int rel_pos = i - j;
        if (rel_pos >= 0 && rel_pos < seqLen) {
            // è®¡ç®—ç›¸å¯¹ä½ç½®ç¼–ç 
            // ...
        }
    }
}
```

## å±‚å½’ä¸€åŒ–å®ç°

### 1. å±‚å½’ä¸€åŒ–Kernel
```cpp
__global__ void layerNormKernel(float *input, float *output,
                                float *gamma, float *beta,
                                int batchSize, int seqLen, int d_model) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int dimIdx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || dimIdx >= d_model) {
        return;
    }
    
    // è®¡ç®—å‡å€¼
    float mean = 0.0f;
    for (int d = 0; d < d_model; d++) {
        mean += input[batchIdx * seqLen * d_model + seqIdx * d_model + d];
    }
    mean /= d_model;
    
    // è®¡ç®—æ–¹å·®
    float variance = 0.0f;
    for (int d = 0; d < d_model; d++) {
        float diff = input[batchIdx * seqLen * d_model + seqIdx * d_model + d] - mean;
        variance += diff * diff;
    }
    variance /= d_model;
    
    // åº”ç”¨å½’ä¸€åŒ–
    int idx = batchIdx * seqLen * d_model + seqIdx * d_model + dimIdx;
    float normalized = (input[idx] - mean) / sqrtf(variance + 1e-6f);
    output[idx] = gamma[dimIdx] * normalized + beta[dimIdx];
}
```

## Transformeræ¶æ„å®ç°

### 1. Transformer Block
```cpp
struct TransformerBlock {
    MultiHeadAttention self_attn;
    float *ffn_weights1, *ffn_weights2;
    float *ffn_bias1, *ffn_bias2;
    float *ln1_gamma, *ln1_beta;
    float *ln2_gamma, *ln2_beta;
    int d_model, d_ff;
};

__global__ void transformerBlockKernel(TransformerBlock *block,
                                      float *input, float *output,
                                      int batchSize, int seqLen) {
    // è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    // å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    // ...
}
```

### 2. å®Œæ•´Transformer
```cpp
class Transformer {
private:
    std::vector<TransformerBlock> layers;
    float *input_embedding;
    float *output_embedding;
    float *pos_encoding;
    
public:
    void forward(float *input, float *output, int batchSize, int seqLen);
    void backward(float *grad_output, float *grad_input);
};
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å†…å­˜è®¿é—®ä¼˜åŒ–
- **æ•°æ®å¸ƒå±€**: ä½¿ç”¨NHWCæ ¼å¼æé«˜å†…å­˜åˆå¹¶è®¿é—®
- **å…±äº«å†…å­˜**: ç¼“å­˜é¢‘ç¹è®¿é—®çš„æ•°æ®
- **å†…å­˜æ± **: å‡å°‘å†…å­˜åˆ†é…å¼€é”€

### 2. è®¡ç®—ä¼˜åŒ–
- **èåˆæ“ä½œ**: å°†å¤šä¸ªæ“ä½œåˆå¹¶åˆ°ä¸€ä¸ªkernelä¸­
- **å‘é‡åŒ–**: ä½¿ç”¨å‘é‡åŒ–æŒ‡ä»¤æé«˜ååé‡
- **å¾ªç¯å±•å¼€**: å‡å°‘å¾ªç¯å¼€é”€

### 3. å¹¶è¡ŒåŒ–ç­–ç•¥
- **åºåˆ—å¹¶è¡Œ**: ä¸åŒåºåˆ—å¹¶è¡Œå¤„ç†
- **å¤´éƒ¨å¹¶è¡Œ**: ä¸åŒæ³¨æ„åŠ›å¤´å¹¶è¡Œè®¡ç®—
- **æ—¶é—´æ­¥å¹¶è¡Œ**: åºåˆ—å†…æ—¶é—´æ­¥å¹¶è¡Œ

## ç¼–è¯‘å’Œè¿è¡Œ

### ç¼–è¯‘å‘½ä»¤
```bash
nvcc -O3 -lcublas -o transformer transformer.cu
```

### è¿è¡Œå‘½ä»¤
```bash
./transformer
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•é…ç½®
- åºåˆ—é•¿åº¦: 512, 1024, 2048
- æ¨¡å‹ç»´åº¦: 512, 768, 1024
- æ³¨æ„åŠ›å¤´æ•°: 8, 12, 16
- æ‰¹å¤„ç†å¤§å°: 1, 4, 8, 16

### æ€§èƒ½æŒ‡æ ‡
- **ååé‡**: æ¯ç§’å¤„ç†çš„tokenæ•°
- **å»¶è¿Ÿ**: å•ä¸ªåºåˆ—çš„å¤„ç†æ—¶é—´
- **å†…å­˜ä½¿ç”¨**: GPUå†…å­˜å ç”¨
- **è®¡ç®—æ•ˆç‡**: FLOPSåˆ©ç”¨ç‡

## å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. æ•°å€¼ç¨³å®šæ€§
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- æ£€æŸ¥NaN/Infå€¼

### 2. å†…å­˜ä¸è¶³
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- å‡å°‘æ‰¹å¤„ç†å¤§å°
- ä¼˜åŒ–å†…å­˜å¸ƒå±€

### 3. æ€§èƒ½ç“¶é¢ˆ
- ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·è¯†åˆ«ç“¶é¢ˆ
- ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- è°ƒæ•´çº¿ç¨‹å—å¤§å°

## ä¸‹ä¸€æ­¥
æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹ æœ€æ–°çš„LLM CUDA kernelå®šåˆ¶ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬Flash Attentionã€Paged Attentionç­‰å‰æ²¿æŠ€æœ¯ã€‚

## ç»ƒä¹ 
1. å®ç°ä¸åŒæ³¨æ„åŠ›å˜ä½“(Relative, Local, Sparse)
2. æ·»åŠ dropoutå’Œæ®‹å·®è¿æ¥
3. å®ç°å®Œæ•´çš„Transformerè®­ç»ƒå¾ªç¯
4. ä½¿ç”¨TensorRTä¼˜åŒ–æ¨ç†æ€§èƒ½

## å‚è€ƒèµ„æ–™
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [CUDA Convolution Implementation](https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/)
- [cuDNN Library](https://docs.nvidia.com/deeplearning/cudnn/)
- [CNN Architecture Design](https://arxiv.org/abs/1512.03385)
- [Transformer Architecture Visualization](https://jalammar.github.io/illustrated-transformer/)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Flash Attention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [Paged Attention: From Interface to Implementation](https://arxiv.org/abs/2309.06180)
- [CUDA Performance Optimization](https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/)
- [NVIDIA Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/)
- [TensorRT Optimization Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)

---

## ğŸ“ ç›¸å…³æ–‡ä»¶å¿«é€Ÿé“¾æ¥
æœ¬æ•™ç¨‹åŒ…å«ä»¥ä¸‹ç›¸å…³ç¨‹åºæ–‡ä»¶ï¼Œç‚¹å‡»å³å¯æŸ¥çœ‹ï¼š

### ğŸš€ ç¤ºä¾‹ç¨‹åº
- [`self_attention.cu`](self_attention.cu) - åŸºç¡€è‡ªæ³¨æ„åŠ›å®ç°
- [`multi_head_attention.cu`](multi_head_attention.cu) - å¤šå¤´æ³¨æ„åŠ›å®ç°
- [`transformer_block.cu`](transformer_block.cu) - Transformerå—å®ç°
- [`transformer.cu`](transformer.cu) - å®Œæ•´Transformerå®ç°

### ğŸ“Š æ€§èƒ½åˆ†æå·¥å…·
- ä½¿ç”¨`nvprof`è¿›è¡Œå‘½ä»¤è¡Œæ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Systemsè¿›è¡Œç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Computeè¿›è¡Œkernelçº§æ€§èƒ½åˆ†æ

### ğŸ”§ ä¼˜åŒ–æŠ€å·§
- å…±äº«å†…å­˜ä¼˜åŒ–
- å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
- è®¡ç®—èåˆä¼˜åŒ–
- å¹¶è¡ŒåŒ–ç­–ç•¥ä¼˜åŒ–
