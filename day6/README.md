# Day 6: æœ€æ–°LLM CUDA Kernelå®šåˆ¶ä¼˜åŒ– - å‰æ²¿æŠ€æœ¯å®æˆ˜

## æ¦‚è¿°
ä»Šå¤©æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æœ€æ–°çš„LLM CUDA kernelå®šåˆ¶ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬Flash Attentionã€Paged Attentionã€Grouped Query Attentionç­‰å‰æ²¿æŠ€æœ¯ã€‚è¿™äº›ä¼˜åŒ–æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ï¼Œæ˜¯å½“å‰AIé¢†åŸŸçš„çƒ­ç‚¹ç ”ç©¶æ–¹å‘ã€‚

## å­¦ä¹ ç›®æ ‡
- ç†è§£Flash Attentionçš„åŸç†å’ŒCUDAå®ç°
- æŒæ¡Paged Attentionçš„å†…å­˜ç®¡ç†ä¼˜åŒ–
- å­¦ä¼šGrouped Query Attentionçš„å®ç°
- ç†è§£ç¨€ç–æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›
- æŒæ¡æœ€æ–°çš„Tensor Coreä¼˜åŒ–æŠ€æœ¯

## Flash Attentionå®ç°

### 1. Flash AttentionåŸç†
Flash Attentioné€šè¿‡åˆ†å—è®¡ç®—å’Œåœ¨çº¿softmaxæ¥å‡å°‘å†…å­˜å ç”¨ï¼Œå®ç°O(N)çš„å†…å­˜å¤æ‚åº¦ï¼š

```
ç®—æ³•æ ¸å¿ƒæ€æƒ³ï¼š
1. å°†è¾“å…¥åºåˆ—åˆ†å—å¤„ç†
2. åœ¨çº¿è®¡ç®—softmaxï¼Œé¿å…å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
3. ä½¿ç”¨æ•°å€¼ç¨³å®šçš„ç®—æ³•é¿å…æº¢å‡º
```

### 2. Flash Attention CUDAå®ç°
```cpp
__global__ void flashAttentionKernel(float *Q, float *K, float *V, float *output,
                                    int batchSize, int seqLen, int d_k, int d_v,
                                    int blockSize) {
    __shared__ float s_Q[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_K[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_V[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_softmax[BLOCK_SIZE];
    
    // å°†4ç»´ä¿¡æ¯ç¼–ç åˆ°3ç»´ä¸­
    int batchIdx = blockIdx.x;
    int headIdx = blockIdx.y;
    int blockIdx_x = blockIdx.z / batchSize;  // ä½¿ç”¨zç»´åº¦çš„é«˜ä½
    int blockIdx_y = blockIdx.z % batchSize;  // ä½¿ç”¨zç»´åº¦çš„ä½ä½
    
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    // åˆ†å—åŠ è½½Q, K, V
    if (tx < BLOCK_SIZE && ty < BLOCK_SIZE) {
        int q_idx = batchIdx * seqLen * d_k + (blockIdx_x * BLOCK_SIZE + tx) * d_k + 
                    (headIdx * d_k + ty);
        int k_idx = batchIdx * seqLen * d_k + (blockIdx_y * BLOCK_SIZE + tx) * d_k + 
                    (headIdx * d_k + ty);
        int v_idx = batchIdx * seqLen * d_v + (blockIdx_y * BLOCK_SIZE + tx) * d_v + 
                    (headIdx * d_v + ty);
        
        if (blockIdx_x * BLOCK_SIZE + tx < seqLen && headIdx * d_k + ty < d_k) {
            s_Q[ty][tx] = Q[q_idx];
        } else {
            s_Q[ty][tx] = 0.0f;
        }
        
        if (blockIdx_y * BLOCK_SIZE + tx < seqLen && headIdx * d_k + ty < d_k) {
            s_K[ty][tx] = K[k_idx];
        } else {
            s_K[ty][tx] = 0.0f;
        }
        
        if (blockIdx_y * BLOCK_SIZE + tx < seqLen && headIdx * d_v + ty < d_v) {
            s_V[ty][tx] = V[v_idx];
        } else {
            s_V[ty][tx] = 0.0f;
        }
    }
    __syncthreads();
    
    // è®¡ç®—åˆ†å—æ³¨æ„åŠ›åˆ†æ•°
    float local_sum = 0.0f;
    float local_max = -INFINITY;
    
    for (int i = 0; i < BLOCK_SIZE; i++) {
        float score = 0.0f;
        for (int j = 0; j < BLOCK_SIZE; j++) {
            score += s_Q[ty][j] * s_K[i][j];
        }
        score /= sqrtf(d_k);
        
        // åœ¨çº¿softmaxè®¡ç®—
        local_max = max(local_max, score);
        local_sum += expf(score - local_max);
    }
    
    // åä½œè®¡ç®—å…¨å±€æœ€å¤§å€¼
    __shared__ float s_max[BLOCK_SIZE];
    
    s_max[ty] = local_max;
    __syncthreads();
    
    // è§„çº¦æ±‚å…¨å±€æœ€å¤§å€¼
    for (int stride = BLOCK_SIZE/2; stride > 0; stride >>= 1) {
        if (ty < stride) {
            s_max[ty] = max(s_max[ty], s_max[ty + stride]);
        }
        __syncthreads();
    }
    
    float global_max = s_max[0];
    __syncthreads();
    
    // é‡æ–°è®¡ç®—softmax
    for (int i = 0; i < BLOCK_SIZE; i++) {
        float score = 0.0f;
        for (int j = 0; j < BLOCK_SIZE; j++) {
            score += s_Q[ty][j] * s_K[i][j];
        }
        score = (score - global_max) / sqrtf(d_k);
        s_softmax[i] = expf(score);
    }
    
    // é‡æ–°è®¡ç®—å…¨å±€å’Œç”¨äºå½’ä¸€åŒ–
    float global_sum = 0.0f;
    for (int i = 0; i < BLOCK_SIZE; i++) {
        global_sum += s_softmax[i];
    }
    
    // å½’ä¸€åŒ–softmax
    for (int i = 0; i < BLOCK_SIZE; i++) {
        s_softmax[i] /= global_sum;
    }
    
    // è®¡ç®—è¾“å‡º
    for (int v = 0; v < BLOCK_SIZE; v++) {
        float weighted_sum = 0.0f;
        for (int i = 0; i < BLOCK_SIZE; i++) {
            weighted_sum += s_softmax[i] * s_V[i][v];
        }
        
        int out_idx = batchIdx * seqLen * d_v + (blockIdx_x * BLOCK_SIZE + tx) * d_v + 
                      (headIdx * d_v + v);
        if (blockIdx_x * BLOCK_SIZE + tx < seqLen && headIdx * d_v + v < d_v) {
            output[out_idx] = weighted_sum;
        }
    }
}
```

**ç¼–è¯‘å’Œè¿è¡Œ:**
```bash
# ç¼–è¯‘
nvcc -O3 -o flash_attention flash_attention.cu

# è¿è¡Œ
./flash_attention
```

### 3. Flash Attentionä¼˜åŒ–æŠ€å·§
- **åˆ†å—å¤§å°ä¼˜åŒ–**: æ ¹æ®GPUæ¶æ„é€‰æ‹©æœ€ä¼˜åˆ†å—å¤§å°
- **å…±äº«å†…å­˜ä½¿ç”¨**: æœ€å¤§åŒ–åˆ©ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
- **æ•°å€¼ç¨³å®šæ€§**: ä½¿ç”¨åœ¨çº¿softmaxé¿å…æ•°å€¼æº¢å‡º

## Grouped Query Attention (GQA)

### 1. GQAåŸç†
GQAé€šè¿‡åˆ†ç»„æŸ¥è¯¢æ¥å‡å°‘è®¡ç®—é‡å’Œå†…å­˜å ç”¨ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æå‡æ•ˆç‡ï¼š

```
GQAç‰¹ç‚¹ï¼š
1. å°†æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œæ¯ç»„å…±äº«é”®å€¼
2. å‡å°‘KVç¼“å­˜å¤§å°
3. ä¿æŒæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒåŠŸèƒ½
4. é€‚ç”¨äºé•¿åºåˆ—åœºæ™¯
```

### 2. GQA CUDAå®ç°
```cpp
__global__ void groupedQueryAttentionKernel(float *Q, float *K, float *V, float *output,
                                           int batchSize, int seqLen, int d_k, int d_v,
                                           int numHeads, int numGroups) {
    int batchIdx = blockIdx.x;
    int groupIdx = blockIdx.y;
    int seqIdx = blockIdx.z;
    int headIdx = threadIdx.x;
    
    if (batchIdx >= batchSize || groupIdx >= numGroups || seqIdx >= seqLen || 
        headIdx >= (numHeads / numGroups)) {
        return;
    }
    
    // è®¡ç®—å…¨å±€å¤´éƒ¨ç´¢å¼•
    int globalHeadIdx = groupIdx * (numHeads / numGroups) + headIdx;
    
    // è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    float attention_score = 0.0f;
    for (int k = 0; k < d_k; k++) {
        float q_val = Q[batchIdx * seqLen * numHeads * d_k + seqIdx * numHeads * d_k + 
                        globalHeadIdx * d_k + k];
        float k_val = K[batchIdx * seqLen * numGroups * d_k + seqIdx * numGroups * d_k + 
                        groupIdx * d_k + k];
        attention_score += q_val * k_val;
    }
    attention_score /= sqrtf(d_k);
    
    // åº”ç”¨softmax
    attention_score = tanhf(attention_score);
    
    // è®¡ç®—åŠ æƒè¾“å‡º
    for (int v = 0; v < d_v; v++) {
        float v_val = V[batchIdx * seqLen * numGroups * d_v + seqIdx * numGroups * d_v + 
                        groupIdx * d_v + v];
        float weighted_val = attention_score * v_val;
        
        int out_idx = batchSize * seqLen * numHeads * d_v + seqIdx * numHeads * d_v + 
                      globalHeadIdx * d_v + v;
        output[out_idx] = weighted_val;
    }
}
```

**ç¼–è¯‘å’Œè¿è¡Œ:**
```bash
# ç¼–è¯‘
nvcc -O3 -o grouped_query_attention grouped_query_attention.cu

# è¿è¡Œ
./grouped_query_attention
```

## æ··åˆç²¾åº¦æ³¨æ„åŠ›ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦åŸç†
åœ¨ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œæ¿€æ´»å€¼éƒ½ä½¿ç”¨ FP32ï¼ˆå•ç²¾åº¦æµ®ç‚¹æ•°ï¼‰ è¿›è¡Œå­˜å‚¨å’Œè®¡ç®—ã€‚FP32 æä¾›äº†å¹¿æ³›çš„æ•°å€¼èŒƒå›´å’Œé«˜ç²¾åº¦ï¼Œè¶³ä»¥æ»¡è¶³å¤§å¤šæ•°ç§‘å­¦è®¡ç®—çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„çˆ†ç‚¸å¼å¢é•¿ï¼ˆä¾‹å¦‚ GPT-4ã€Llama 3 ç­‰ï¼‰ï¼Œä½¿ç”¨ FP32 å¸¦æ¥äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š

- å·¨å¤§çš„æ˜¾å­˜ï¼ˆVRAMï¼‰å ç”¨ï¼šæ¯ä¸ª FP32 å˜é‡éœ€è¦ 4 ä¸ªå­—èŠ‚ã€‚ä¸€ä¸ªæ‹¥æœ‰æ•°åäº¿ç”šè‡³ä¸Šä¸‡äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå…¶å‚æ•°æœ¬èº«å°±éœ€è¦æ•°ç™¾ GB çš„æ˜¾å­˜ã€‚
- å†—é•¿çš„è®­ç»ƒæ—¶é—´ï¼šFP32 è®¡ç®—éœ€è¦æ›´å¤šçš„æ™¶ä½“ç®¡å’Œèƒ½è€—ï¼Œå¯¼è‡´è®¡ç®—é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚

FP16ï¼ˆåŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰ åº”è¿è€Œç”Ÿã€‚å®ƒä½¿ç”¨ 2 ä¸ªå­—èŠ‚æ¥å­˜å‚¨æ•°æ®ï¼Œå…¶å­˜å‚¨ç©ºé—´æ˜¯ FP32 çš„ä¸€åŠï¼Œä½†ä»£ä»·æ˜¯æ•°å€¼èŒƒå›´æ›´çª„ï¼Œç²¾åº¦ä¹Ÿæ›´ä½ã€‚æ··åˆç²¾åº¦è®­ç»ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯å·§å¦™åœ°ç»“åˆ FP16 å’Œ FP32 çš„ä¼˜åŠ¿ï¼š

- ç”¨ FP16 èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿè®¡ç®—ï¼šåœ¨è®­ç»ƒçš„å¤§éƒ¨åˆ†æ—¶é—´é‡Œï¼Œå°†æ¨¡å‹å‚æ•°å’Œæ¿€æ´»å€¼å­˜å‚¨ä¸º FP16ã€‚è¿™èƒ½ç«‹å³å°†æ˜¾å­˜å ç”¨å‡åŠï¼Œä»è€Œå…è®¸ä½ è®­ç»ƒæ›´å¤§ã€æ›´å¤æ‚çš„æ¨¡å‹ã€‚
- ç”¨ FP32 ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼šåœ¨æŸäº›å¯¹ç²¾åº¦æ•æ„Ÿçš„æ“ä½œä¸­ï¼Œä¾‹å¦‚æ¢¯åº¦çš„ç´¯åŠ ï¼Œæˆ–è€… Softmax å‡½æ•°ï¼ˆåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­è‡³å…³é‡è¦ï¼‰ï¼Œç»§ç»­ä½¿ç”¨ FP32 æ¥é¿å… ä¸‹æº¢ï¼ˆunderflowï¼‰ æˆ– æº¢å‡ºï¼ˆoverflowï¼‰ã€‚ä¸‹æº¢æ˜¯æŒ‡æ•°å€¼è¿‡å°è¢«èˆå…¥ä¸ºé›¶ï¼Œè€Œæº¢å‡ºåˆ™æ˜¯æ•°å€¼è¿‡å¤§è¶…å‡º FP16 çš„è¡¨ç¤ºèŒƒå›´ã€‚

æ··åˆç²¾åº¦ä½¿ç”¨FP16è¿›è¡Œè®¡ç®—ï¼ŒFP32è¿›è¡Œç´¯åŠ ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æå‡æ€§èƒ½ï¼š

```
ä¼˜åŠ¿ï¼š
1. å†…å­˜å¸¦å®½å‡åŠ
2. è®¡ç®—é€Ÿåº¦æå‡
3. æ”¯æŒTensor CoreåŠ é€Ÿ
4. æ•°å€¼ç¨³å®šæ€§ä¿æŒ
```

### 2. æ··åˆç²¾åº¦å®ç°
```cpp
__global__ void mixedPrecisionAttentionKernel(half *Q, half *K, half *V, 
                                             float *output, float *scale,
                                             int batchSize, int seqLen, int d_k, int d_v) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int tx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || headIdx >= d_k) {
        return;
    }
    
    // ä½¿ç”¨halfç²¾åº¦è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    half attention_score = __float2half(0.0f);
    for (int k = 0; k < d_k; k++) {
        half q_val = Q[batchIdx * seqLen * d_k + seqIdx * d_k + k];
        half k_val = K[batchIdx * seqLen * d_k + headIdx * d_k + k];
        attention_score = __hadd(attention_score, __hmul(q_val, k_val));
    }
    
    // åº”ç”¨ç¼©æ”¾
    attention_score = __hmul(attention_score, __float2half(scale[0]));
    
    // è®¡ç®—è¾“å‡º
    for (int v = 0; v < d_v; v++) {
        half v_val = V[batchIdx * seqLen * d_v + headIdx * d_v + v];
        half weighted_val = __hmul(attention_score, v_val);
        
        int out_idx = batchIdx * seqLen * d_v + seqIdx * d_v + v;
        output[out_idx] = __half2float(weighted_val);
    }
}
```

**ç¼–è¯‘å’Œè¿è¡Œ:**
```bash
# ç¼–è¯‘ (éœ€è¦æ”¯æŒhalfç²¾åº¦çš„GPU)
nvcc -O3 -o mixed_precision_attention mixed_precision_attention.cu

# è¿è¡Œ
./mixed_precision_attention
```

## ç¨€ç–æ³¨æ„åŠ›ä¼˜åŒ–

### 1. ç¨€ç–æ³¨æ„åŠ›åŸç†
ç¨€ç–æ³¨æ„åŠ›é€šè¿‡åªè®¡ç®—éƒ¨åˆ†æ³¨æ„åŠ›åˆ†æ•°æ¥å‡å°‘è®¡ç®—é‡ï¼š

```
ç¨€ç–æ¨¡å¼ï¼š
1. å±€éƒ¨æ³¨æ„åŠ›: åªå…³æ³¨é‚»è¿‘ä½ç½®
2. éšæœºæ³¨æ„åŠ›: éšæœºé€‰æ‹©ä½ç½®
3. åˆ†å±‚æ³¨æ„åŠ›: ä¸åŒå±‚ä½¿ç”¨ä¸åŒç¨€ç–æ¨¡å¼
```

### 2. ç¨€ç–æ³¨æ„åŠ›å®ç°
```cpp
__global__ void sparseAttentionKernel(float *Q, float *K, float *V, float *output,
                                     int *sparse_indices, int *sparse_offsets,
                                     int batchSize, int seqLen, int d_k, int d_v,
                                     int max_sparse_connections) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int tx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || headIdx >= d_k) {
        return;
    }
    
    // è·å–ç¨€ç–è¿æ¥ä¿¡æ¯
    int start_idx = sparse_offsets[batchIdx * seqLen + seqIdx];
    int end_idx = sparse_offsets[batchIdx * seqLen + seqIdx + 1];
    int num_connections = end_idx - start_idx;
    
    if (num_connections > max_sparse_connections) {
        num_connections = max_sparse_connections;
    }
    
    // è®¡ç®—ç¨€ç–æ³¨æ„åŠ›åˆ†æ•°
    float max_score = -INFINITY;
    float sum_exp = 0.0f;
    
    for (int i = 0; i < num_connections; i++) {
        int target_idx = sparse_indices[start_idx + i];
        if (target_idx < 0 || target_idx >= seqLen) continue;
        
        float score = 0.0f;
        for (int k = 0; k < d_k; k++) {
            float q_val = Q[batchIdx * seqLen * d_k + seqIdx * d_k + k];
            float k_val = K[batchIdx * seqLen * d_k + target_idx * d_k + k];
            score += q_val * k_val;
        }
        score /= sqrtf(d_k);
        
        max_score = max(max_score, score);
        sum_exp += expf(score - max_score);
    }
    
    // åº”ç”¨softmax
    for (int i = 0; i < num_connections; i++) {
        int target_idx = sparse_indices[start_idx + i];
        if (target_idx < 0 || target_idx >= seqLen) continue;
        
        float score = 0.0f;
        for (int k = 0; k < d_k; k++) {
            float q_val = Q[batchIdx * seqLen * d_k + seqIdx * d_k + k];
            float k_val = K[batchIdx * seqLen * d_k + target_idx * d_k + k];
            score += q_val * k_val;
        }
        score = (score - max_score) / sqrtf(d_k);
        float attention_weight = expf(score) / sum_exp;
        
        // è®¡ç®—åŠ æƒè¾“å‡º
        for (int v = 0; v < d_v; v++) {
            float v_val = V[batchIdx * seqLen * d_v + target_idx * d_v + v];
            float weighted_val = attention_weight * v_val;
            
            int out_idx = batchIdx * seqLen * d_v + seqIdx * d_v + v;
            atomicAdd(&output[out_idx], weighted_val);
        }
    }
}
```

**ç¼–è¯‘å’Œè¿è¡Œ:**
```bash
# ç¼–è¯‘
nvcc -O3 -o sparse_attention sparse_attention.cu

# è¿è¡Œ
./sparse_attention
```

## æœ€æ–°Tensor Coreä¼˜åŒ–

### 1. Blackwellæ¶æ„æ”¯æŒ
æœ€æ–°çš„Blackwell GPUæ”¯æŒæ–°çš„Tensor CoreæŒ‡ä»¤ï¼Œå¦‚tcgen05.mmaï¼š

```cpp
// ä½¿ç”¨æœ€æ–°çš„Tensor CoreæŒ‡ä»¤
__global__ void blackwellTensorCoreKernel(half *A, half *B, float *C,
                                         int M, int N, int K) {
    // ä½¿ç”¨tcgen05.mmaæŒ‡ä»¤
    // æ³¨æ„ï¼šè¿™æ˜¯æ¦‚å¿µæ€§ä»£ç ï¼Œå®é™…æŒ‡ä»¤å¯èƒ½ä¸åŒ
    
    // åŠ è½½æ•°æ®åˆ°Tensor Core
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;
    
    // ä½¿ç”¨æ–°çš„Tensor CoreæŒ‡ä»¤
    // tcgen05.mma(a_frag, b_frag, c_frag);
    
    // å­˜å‚¨ç»“æœ
    wmma::store_matrix_sync(C, c_frag, 16, wmma::mem_row_major);
}
```

### 2. æ··åˆç²¾åº¦ä¼˜åŒ–
```cpp
// æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–
__global__ void mixedPrecisionAttentionKernel(half *Q, half *K, half *V, 
                                             float *output, float *scale,
                                             int batchSize, int seqLen, int d_k, int d_v) {
    int batchIdx = blockIdx.x;
    int seqIdx = blockIdx.y;
    int headIdx = blockIdx.z;
    int tx = threadIdx.x;
    
    if (batchIdx >= batchSize || seqIdx >= seqLen || headIdx >= d_k) {
        return;
    }
    
    // ä½¿ç”¨halfç²¾åº¦è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    half attention_score = __float2half(0.0f);
    for (int k = 0; k < d_k; k++) {
        half q_val = Q[batchIdx * seqLen * d_k + seqIdx * d_k + k];
        half k_val = K[batchIdx * seqLen * d_k + headIdx * d_k + k];
        attention_score = __hadd(attention_score, __hmul(q_val, k_val));
    }
    
    // åº”ç”¨ç¼©æ”¾
    attention_score = __hmul(attention_score, __float2half(scale[0]));
    
    // è®¡ç®—è¾“å‡º
    for (int v = 0; v < d_v; v++) {
        half v_val = V[batchIdx * seqLen * d_v + headIdx * d_v + v];
        half weighted_val = __hmul(attention_score, v_val);
        
        int out_idx = batchIdx * seqLen * d_v + seqIdx * d_v + v;
        output[out_idx] = __half2float(weighted_val);
    }
}
```

## é’ˆå¯¹ä¸åŒGPUæ¶æ„ç¼–è¯‘

```bash
# RTX 30ç³»åˆ— (Ampere)
nvcc -O3 -o flash_attention flash_attention.cu

# RTX 40ç³»åˆ— (Ada Lovelace)  
nvcc -O3 -o flash_attention flash_attention.cu

# H100/H200 (Hopper)
nvcc -O3 -o flash_attention flash_attention.cu
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å†…å­˜è®¿é—®ä¼˜åŒ–
- **åˆ†å—å¤„ç†**: æ ¹æ®GPUå†…å­˜å±‚æ¬¡ç»“æ„ä¼˜åŒ–æ•°æ®è®¿é—®
- **é¢„å–æŠ€æœ¯**: ä½¿ç”¨å¼‚æ­¥å†…å­˜ä¼ è¾“é‡å è®¡ç®—å’Œä¼ è¾“
- **å†…å­˜æ± **: å‡å°‘åŠ¨æ€å†…å­˜åˆ†é…å¼€é”€

### 2. è®¡ç®—ä¼˜åŒ–
- **æŒ‡ä»¤èåˆ**: å°†å¤šä¸ªæ“ä½œåˆå¹¶åˆ°å•ä¸ªkernelä¸­
- **å¾ªç¯å±•å¼€**: å‡å°‘å¾ªç¯å¼€é”€ï¼Œæé«˜æŒ‡ä»¤çº§å¹¶è¡Œæ€§
- **å‘é‡åŒ–**: ä½¿ç”¨å‘é‡åŒ–æŒ‡ä»¤æé«˜ååé‡

### 3. å¹¶è¡ŒåŒ–ä¼˜åŒ–
- **å¤šæµå¹¶è¡Œ**: ä½¿ç”¨å¤šä¸ªCUDAæµé‡å æ‰§è¡Œ
- **åŠ¨æ€å¹¶è¡Œ**: åœ¨GPUä¸ŠåŠ¨æ€å¯åŠ¨å­kernel
- **åä½œç»„**: ä½¿ç”¨åä½œç»„ä¼˜åŒ–çº¿ç¨‹é—´é€šä¿¡

## ç¼–è¯‘å’Œè¿è¡Œ

### ç¼–è¯‘å‘½ä»¤
```bash
# é’ˆå¯¹æœ€æ–°æ¶æ„ç¼–è¯‘
nvcc -O3 -o llm_optimization llm_optimization.cu

# å¯ç”¨Tensor Core
nvcc -O3 -lcublas -o llm_optimization llm_optimization.cu
```

### è¿è¡Œå‘½ä»¤
```bash
./llm_optimization
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•é…ç½®
- åºåˆ—é•¿åº¦: 1024, 2048, 4096, 8192
- æ¨¡å‹ç»´åº¦: 512, 768, 1024, 2048
- æ³¨æ„åŠ›å¤´æ•°: 8, 12, 16, 32
- æ‰¹å¤„ç†å¤§å°: 1, 4, 8, 16, 32

### æ€§èƒ½æŒ‡æ ‡
- **ååé‡**: æ¯ç§’å¤„ç†çš„tokenæ•°
- **å»¶è¿Ÿ**: å•ä¸ªåºåˆ—çš„å¤„ç†æ—¶é—´
- **å†…å­˜ä½¿ç”¨**: GPUå†…å­˜å ç”¨å’Œåˆ©ç”¨ç‡
- **è®¡ç®—æ•ˆç‡**: Tensor Coreåˆ©ç”¨ç‡

## å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. æ•°å€¼ç¨³å®šæ€§
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæé«˜æ•°å€¼ç¨³å®šæ€§
- å®ç°æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- ä½¿ç”¨ç¨³å®šçš„softmaxç®—æ³•

### 2. å†…å­˜ç®¡ç†
- å®ç°é«˜æ•ˆçš„åˆ†é¡µç®¡ç†
- ä½¿ç”¨å†…å­˜æ± å‡å°‘ç¢ç‰‡
- ä¼˜åŒ–KVç¼“å­˜ç­–ç•¥

### 3. æ€§èƒ½ç“¶é¢ˆ
- ä½¿ç”¨Nsightå·¥å…·åˆ†ææ€§èƒ½ç“¶é¢ˆ
- ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- è°ƒæ•´kernelé…ç½®å‚æ•°

## ä¸‹ä¸€æ­¥
æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹ CUDAæ€§èƒ½è°ƒä¼˜çš„é«˜çº§æŠ€å·§ï¼ŒåŒ…æ‹¬å†…å­˜ä¼˜åŒ–ã€æŒ‡ä»¤ä¼˜åŒ–å’Œæ¶æ„ç‰¹å®šçš„ä¼˜åŒ–ã€‚

## ç»ƒä¹ 
1. å®ç°ä¸åŒç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
2. ä¼˜åŒ–Flash Attentionçš„åˆ†å—ç­–ç•¥
3. å®ç°åŠ¨æ€åºåˆ—é•¿åº¦çš„Paged Attention
4. ä½¿ç”¨æœ€æ–°Tensor CoreæŒ‡ä»¤ä¼˜åŒ–æ€§èƒ½

## å‚è€ƒèµ„æ–™
- [Flash Attention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [Paged Attention: From Interface to Implementation](https://arxiv.org/abs/2309.06180)
- [Grouped Query Attention](https://arxiv.org/abs/2305.13245)
- [Efficient Memory Management for Large Language Model Serving](https://arxiv.org/abs/2309.06180)
- [Sparse Attention with Linear Complexity](https://arxiv.org/abs/2004.05150)
- [NVIDIA Blackwell Architecture](https://www.nvidia.com/en-us/data-center/blackwell/)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Tensor Core Programming](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
- [Claude 3 Opus Technical Report](https://arxiv.org/abs/2507.10789)
- [NVIDIA Transformer Engine](https://docs.nvidia.com/deeplearning/transformer-engine/)
- [TensorRT Optimization Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)
- [CUDA Performance Optimization](https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/)
- [GPU Memory Management](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
- [Attention Optimization Techniques](https://developer.nvidia.com/blog/optimizing-transformer-models-for-inference/)
- [Large Language Model Optimization](https://developer.nvidia.com/blog/optimizing-large-language-models-for-inference/)

---

## ğŸ“ ç›¸å…³æ–‡ä»¶å¿«é€Ÿé“¾æ¥
æœ¬æ•™ç¨‹åŒ…å«ä»¥ä¸‹ç›¸å…³ç¨‹åºæ–‡ä»¶ï¼Œç‚¹å‡»å³å¯æŸ¥çœ‹ï¼š

### ğŸš€ ç¤ºä¾‹ç¨‹åº
- [`flash_attention.cu`](flash_attention.cu) - Flash Attentionå®ç°
- [`paged_attention.cu`](paged_attention.cu) - Paged Attentionå®ç°
- [`grouped_query_attention.cu`](grouped_query_attention.cu) - Grouped Query Attentionå®ç°
- [`sparse_attention.cu`](sparse_attention.cu) - ç¨€ç–æ³¨æ„åŠ›å®ç°
- [`mixed_precision_attention.cu`](mixed_precision_attention.cu) - æ··åˆç²¾åº¦æ³¨æ„åŠ›å®ç°

### ğŸ“Š æ€§èƒ½åˆ†æå·¥å…·
- ä½¿ç”¨`nvprof`è¿›è¡Œå‘½ä»¤è¡Œæ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Systemsè¿›è¡Œç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Computeè¿›è¡Œkernelçº§æ€§èƒ½åˆ†æ

### ğŸ”§ ä¼˜åŒ–æŠ€å·§
- Flash Attentionåˆ†å—ä¼˜åŒ–
- Paged Attentionå†…å­˜ç®¡ç†
- ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ä¼˜åŒ–
- æ··åˆç²¾åº¦è®¡ç®—ä¼˜åŒ–
- Tensor CoreæŒ‡ä»¤ä¼˜åŒ–
