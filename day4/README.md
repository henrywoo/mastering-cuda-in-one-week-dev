# Day 4: å·ç§¯ç¥ç»ç½‘ç»œ(CNN) - CUDAæ·±åº¦å­¦ä¹ å®æˆ˜

## æ¦‚è¿°
ä»Šå¤©æˆ‘ä»¬å°†å®ç°å·ç§¯ç¥ç»ç½‘ç»œ(CNN)çš„æ ¸å¿ƒæ“ä½œï¼šå·ç§¯å±‚å’Œå‰å‘ä¼ æ’­ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„æ“ä½œä¹‹ä¸€ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•é«˜æ•ˆåœ°åœ¨GPUä¸Šå®ç°å·ç§¯è¿ç®—ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å°†äº†è§£NVIDIAæä¾›çš„æ·±åº¦å­¦ä¹ åŠ é€Ÿåº“cuDNNï¼Œå®ƒæä¾›äº†é«˜åº¦ä¼˜åŒ–çš„å·ç§¯ç®—æ³•å®ç°ã€‚

## å­¦ä¹ ç›®æ ‡
- ç†è§£å·ç§¯è¿ç®—çš„æ•°å­¦åŸç†å’Œå®ç°
- æŒæ¡2Då·ç§¯çš„CUDAå®ç°æŠ€å·§
- å­¦ä¼šä½¿ç”¨å…±äº«å†…å­˜ä¼˜åŒ–å·ç§¯æ“ä½œ
- ç†è§£ä¸åŒå·ç§¯ç®—æ³•çš„æ€§èƒ½ç‰¹ç‚¹
- æŒæ¡CNNå‰å‘ä¼ æ’­çš„å®ç°
- äº†è§£cuDNNåº“çš„ä½¿ç”¨å’Œä¼˜åŒ–

## å·ç§¯è¿ç®—åŸºç¡€

### 1. æ•°å­¦å®šä¹‰
å¯¹äºè¾“å…¥ç‰¹å¾å›¾ I (HÃ—WÃ—C) å’Œå·ç§¯æ ¸ K (KhÃ—KwÃ—CÃ—F)ï¼Œè¾“å‡ºç‰¹å¾å›¾ O (H'Ã—W'Ã—F) çš„è®¡ç®—ï¼š
```
O[h][w][f] = Î£(I[h+kh][w+kw][c] * K[kh][kw][c][f])
```

å…¶ä¸­ï¼š
- H', W' = (H - Kh + 2*P) / S + 1 (è€ƒè™‘å¡«å……På’Œæ­¥é•¿S)
- kh, kw éå†å·ç§¯æ ¸çš„å°ºå¯¸
- c éå†è¾“å…¥é€šé“æ•°

### 2. å·ç§¯å‚æ•°
- **Kernel Size**: å·ç§¯æ ¸å¤§å° (å¦‚3Ã—3, 5Ã—5)
- **Stride**: æ­¥é•¿ï¼Œæ§åˆ¶è¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸
- **Padding**: å¡«å……ï¼Œä¿æŒè¾“å‡ºå°ºå¯¸
- **Channels**: è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°

## NVIDIA cuDNNåº“

### 1. cuDNNç®€ä»‹
cuDNN (CUDA Deep Neural Network library) æ˜¯NVIDIAæä¾›çš„æ·±åº¦å­¦ä¹ åŠ é€Ÿåº“ï¼ŒåŒ…å«é«˜åº¦ä¼˜åŒ–çš„å·ç§¯ã€æ± åŒ–ã€å½’ä¸€åŒ–ç­‰æ“ä½œï¼š

```cpp
#include <cudnn.h>

// ä½¿ç”¨cuDNNè¿›è¡Œå·ç§¯è¿ç®—
void conv2dCuDNN(float *input, float *filter, float *output,
                  int batchSize, int inChannels, int inHeight, int inWidth,
                  int outChannels, int filterHeight, int filterWidth,
                  int padHeight, int padWidth, int strideHeight, int strideWidth) {
    cudnnHandle_t cudnn;
    cudnnCreate(&cudnn);
    
    // åˆ›å»ºæè¿°ç¬¦
    cudnnTensorDescriptor_t inputDesc, outputDesc;
    cudnnFilterDescriptor_t filterDesc;
    cudnnConvolutionDescriptor_t convDesc;
    
    cudnnCreateTensorDescriptor(&inputDesc);
    cudnnCreateTensorDescriptor(&outputDesc);
    cudnnCreateFilterDescriptor(&filterDesc);
    cudnnCreateConvolutionDescriptor(&convDesc);
    
    // è®¾ç½®è¾“å…¥æè¿°ç¬¦
    cudnnSetTensor4dDescriptor(inputDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT,
                               batchSize, inChannels, inHeight, inWidth);
    
    // è®¾ç½®æ»¤æ³¢å™¨æè¿°ç¬¦
    cudnnSetFilter4dDescriptor(filterDesc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW,
                               outChannels, inChannels, filterHeight, filterWidth);
    
    // è®¾ç½®å·ç§¯æè¿°ç¬¦
    cudnnSetConvolution2dDescriptor(convDesc, padHeight, padWidth,
                                   strideHeight, strideWidth, 1, 1,
                                   CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT);
    
    // è®¾ç½®è¾“å‡ºæè¿°ç¬¦
    int outHeight, outWidth;
    cudnnGetConvolution2dForwardOutputDim(convDesc, inputDesc, filterDesc,
                                         &outHeight, &outWidth);
    cudnnSetTensor4dDescriptor(outputDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT,
                               batchSize, outChannels, outHeight, outWidth);
    
    // é€‰æ‹©æœ€ä¼˜ç®—æ³•
    cudnnConvolutionFwdAlgo_t algo;
    cudnnGetConvolutionForwardAlgorithm(cudnn, inputDesc, filterDesc, convDesc, outputDesc,
                                       CUDNN_CONVOLUTION_FWD_PREFER_FASTEST, 0, &algo);
    
    // è·å–å·¥ä½œç©ºé—´å¤§å°
    size_t workspaceSize = 0;
    cudnnGetConvolutionForwardWorkspaceSize(cudnn, inputDesc, filterDesc, convDesc,
                                           outputDesc, algo, &workspaceSize);
    
    // åˆ†é…å·¥ä½œç©ºé—´
    void *workspace = nullptr;
    if (workspaceSize > 0) {
        cudaMalloc(&workspace, workspaceSize);
    }
    
    // æ‰§è¡Œå·ç§¯
    float alpha = 1.0f, beta = 0.0f;
    cudnnConvolutionForward(cudnn, &alpha, inputDesc, input, filterDesc, filter,
                           convDesc, algo, workspace, workspaceSize,
                           &beta, outputDesc, output);
    
    // æ¸…ç†èµ„æº
    if (workspace) cudaFree(workspace);
    cudnnDestroyTensorDescriptor(inputDesc);
    cudnnDestroyTensorDescriptor(outputDesc);
    cudnnDestroyFilterDescriptor(filterDesc);
    cudnnDestroyConvolutionDescriptor(convDesc);
    cudnnDestroy(cudnn);
}
```

### 2. cuDNNä¼˜åŒ–ç‰¹æ€§
- **ç®—æ³•è‡ªåŠ¨é€‰æ‹©**: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„å·ç§¯ç®—æ³•
- **å†…å­˜ä¼˜åŒ–**: æœ€å°åŒ–å·¥ä½œç©ºé—´ä½¿ç”¨
- **å¤šç²¾åº¦æ”¯æŒ**: æ”¯æŒFP16ã€FP32ã€FP64ç­‰ç²¾åº¦
- **Tensor Coreæ”¯æŒ**: åœ¨æ”¯æŒçš„GPUä¸Šè‡ªåŠ¨ä½¿ç”¨Tensor Core

### 3. cuDNNå·ç§¯ç®—æ³•
```cpp
// ä¸åŒçš„å·ç§¯ç®—æ³•
enum cudnnConvolutionFwdAlgo_t {
    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,      // éšå¼GEMM
    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM, // é¢„ç¼–è¯‘GEMM
    CUDNN_CONVOLUTION_FWD_ALGO_GEMM,               // æ˜¾å¼GEMM
    CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,             // ç›´æ¥å·ç§¯
    CUDNN_CONVOLUTION_FWD_ALGO_FFT,                // FFTå·ç§¯
    CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,         // åˆ†å—FFTå·ç§¯
    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,           // Winogradå·ç§¯
    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED   // éèåˆWinogradå·ç§¯
};

// é€‰æ‹©ç®—æ³•ç­–ç•¥
void selectOptimalAlgorithm(cudnnHandle_t cudnn, cudnnTensorDescriptor_t inputDesc,
                           cudnnFilterDescriptor_t filterDesc, cudnnConvolutionDescriptor_t convDesc,
                           cudnnTensorDescriptor_t outputDesc) {
    // è·å–æ‰€æœ‰å¯ç”¨ç®—æ³•
    int returnedAlgoCount;
    cudnnConvolutionFwdAlgoPerf_t perfResults[8];
    cudnnFindConvolutionForwardAlgorithm(cudnn, inputDesc, filterDesc, convDesc, outputDesc,
                                        8, &returnedAlgoCount, perfResults);
    
    // é€‰æ‹©æœ€ä¼˜ç®—æ³•
    cudnnConvolutionFwdAlgo_t bestAlgo = perfResults[0].algo;
    printf("Best algorithm: %d, time: %f ms, memory: %zu bytes\n",
           bestAlgo, perfResults[0].time, perfResults[0].memory);
    
    // æ‰“å°æ‰€æœ‰ç®—æ³•æ€§èƒ½
    for (int i = 0; i < returnedAlgoCount; i++) {
        printf("Algorithm %d: time=%f ms, memory=%zu bytes, status=%d\n",
               perfResults[i].algo, perfResults[i].time,
               perfResults[i].memory, perfResults[i].status);
    }
}
```

## å®ç°ç‰ˆæœ¬å¯¹æ¯”

### ç‰ˆæœ¬1: åŸºç¡€å®ç° (Global Memory)
```cpp
__global__ void conv2dBasic(float *input, float *kernel, float *output,
                           int H, int W, int C, int Kh, int Kw, int F,
                           int stride, int padding) {
    int h = blockIdx.y * blockDim.y + threadIdx.y;
    int w = blockIdx.x * blockDim.x + threadIdx.x;
    int f = blockIdx.z * blockDim.z + threadIdx.z;
    
    if (h < H && w < W && f < F) {
        float sum = 0.0f;
        
        for (int kh = 0; kh < Kh; kh++) {
            for (int kw = 0; kw < Kw; kw++) {
                for (int c = 0; c < C; c++) {
                    int ih = h * stride + kh - padding;
                    int iw = w * stride + kw - padding;
                    
                    if (ih >= 0 && ih < H && iw >= 0 && iw < W) {
                        sum += input[c * H * W + ih * W + iw] * 
                               kernel[f * C * Kh * Kw + c * Kh * Kw + kh * Kw + kw];
                    }
                }
            }
        }
        
        output[f * H * W + h * W + w] = sum;
    }
}
```

**é—®é¢˜åˆ†æ:**
- å…¨å±€å†…å­˜è®¿é—®ä¸åˆå¹¶
- é‡å¤è®¿é—®è¾“å…¥æ•°æ®
- å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ä½

### ç‰ˆæœ¬2: å…±äº«å†…å­˜ä¼˜åŒ–
```cpp
__global__ void conv2dShared(float *input, float *kernel, float *output,
                            int H, int W, int C, int Kh, int Kw, int F,
                            int stride, int padding) {
    __shared__ float s_input[TILE_H + 2*PADDING][TILE_W + 2*PADDING];
    __shared__ float s_kernel[KERNEL_SIZE];
    
    int tx = threadIdx.x, ty = threadIdx.y;
    int bx = blockIdx.x, by = blockIdx.y;
    int h = by * TILE_H + ty;
    int w = bx * TILE_W + tx;
    
    // åä½œåŠ è½½è¾“å…¥æ•°æ®åˆ°å…±äº«å†…å­˜
    for (int c = 0; c < C; c++) {
        // åŠ è½½å½“å‰tileçš„æ•°æ®
        if (ty < TILE_H && tx < TILE_W) {
            int ih = h * stride - padding;
            int iw = w * stride - padding;
            
            if (ih >= 0 && ih < H && iw >= 0 && iw < W) {
                s_input[ty + PADDING][tx + PADDING] = 
                    input[c * H * W + ih * W + iw];
            } else {
                s_input[ty + PADDING][tx + PADDING] = 0.0f;
            }
        }
        
        // åŠ è½½å·ç§¯æ ¸æ•°æ®
        if (ty < Kh && tx < Kw) {
            s_kernel[ty * Kw + tx] = kernel[c * Kh * Kw + ty * Kw + tx];
        }
        
        __syncthreads();
        
        // è®¡ç®—å·ç§¯
        if (h < H && w < W) {
            float sum = 0.0f;
            for (int kh = 0; kh < Kh; kh++) {
                for (int kw = 0; kw < Kw; kw++) {
                    sum += s_input[ty + kh][tx + kw] * s_kernel[kh * Kw + kw];
                }
            }
            output[c * H * W + h * W + w] += sum;
        }
        
        __syncthreads();
    }
}
```

**ä¼˜åŒ–ç‚¹:**
- ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
- æ•°æ®é‡ç”¨æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
- åä½œåŠ è½½æé«˜å†…å­˜åˆå¹¶è®¿é—®

### ç‰ˆæœ¬3: åˆ†ç¦»å·ç§¯ (Separable Convolution)
```cpp
// æ°´å¹³æ–¹å‘å·ç§¯
__global__ void conv2dHorizontal(float *input, float *kernel, float *output,
                                int H, int W, int C, int Kw) {
    int h = blockIdx.y * blockDim.y + threadIdx.y;
    int w = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (h < H && w < W) {
        for (int c = 0; c < C; c++) {
            float sum = 0.0f;
            for (int kw = 0; kw < Kw; kw++) {
                int iw = w + kw - Kw/2;
                if (iw >= 0 && iw < W) {
                    sum += input[c * H * W + h * W + iw] * kernel[kw];
                }
            }
            output[c * H * W + h * W + w] = sum;
        }
    }
}

// å‚ç›´æ–¹å‘å·ç§¯
__global__ void conv2dVertical(float *input, float *kernel, float *output,
                              int H, int W, int C, int Kh) {
    int h = blockIdx.y * blockDim.y + threadIdx.y;
    int w = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (h < H && w < W) {
        for (int c = 0; c < C; c++) {
            float sum = 0.0f;
            for (int kh = 0; kh < Kh; kh++) {
                int ih = h + kh - Kh/2;
                if (ih >= 0 && ih < H) {
                    sum += input[c * H * W + ih * W + w] * kernel[kh];
                }
            }
            output[c * H * W + h * W + w] = sum;
        }
    }
}
```

**ä¼˜åŠ¿:**
- å°†2Då·ç§¯åˆ†è§£ä¸ºä¸¤ä¸ª1Då·ç§¯
- å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼šO(KhÃ—Kw) â†’ O(Kh + Kw)
- é€‚ç”¨äºå¯åˆ†ç¦»çš„å·ç§¯æ ¸ï¼ˆå¦‚é«˜æ–¯æ ¸ï¼‰

## æ€§èƒ½å¯¹æ¯”åˆ†æ

### 1. è‡ªå®šä¹‰å®ç° vs cuDNN
```cpp
void benchmarkConvolutionMethods(int H, int W, int C, int Kh, int Kw, int F,
                                int stride, int padding, int iterations) {
    // åˆ†é…å†…å­˜
    size_t inputSize = H * W * C * sizeof(float);
    size_t kernelSize = F * C * Kh * Kw * sizeof(float);
    size_t outputSize = H * W * F * sizeof(float);
    
    float *h_input, *h_kernel, *h_output;
    float *d_input, *d_kernel, *d_output;
    
    cudaMallocHost(&h_input, inputSize);
    cudaMallocHost(&h_kernel, kernelSize);
    cudaMallocHost(&h_output, outputSize);
    cudaMalloc(&d_input, inputSize);
    cudaMalloc(&d_kernel, kernelSize);
    cudaMalloc(&d_output, outputSize);
    
    // åˆå§‹åŒ–æ•°æ®
    // ... åˆå§‹åŒ–ä»£ç  ...
    
    // æµ‹è¯•è‡ªå®šä¹‰å®ç°
    dim3 blockDim(TILE_W, TILE_H);
    dim3 gridDim((W + TILE_W - 1) / TILE_W, (H + TILE_H - 1) / TILE_H);
    
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < iterations; i++) {
        conv2dShared<<<gridDim, blockDim>>>(d_input, d_kernel, d_output,
                                           H, W, C, Kh, Kw, F, stride, padding);
    }
    cudaDeviceSynchronize();
    auto end = std::chrono::high_resolution_clock::now();
    auto custom_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    // æµ‹è¯•cuDNNå®ç°
    start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < iterations; i++) {
        conv2dCuDNN(h_input, h_kernel, h_output, 1, C, H, W, F, Kh, Kw,
                    padding, padding, stride, stride);
    }
    auto cudnn_time = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    printf("Custom Implementation: %ld Î¼s\n", custom_time.count() / iterations);
    printf("cuDNN Implementation: %ld Î¼s\n", cudnn_time.count() / iterations);
    printf("Speedup: %.2fx\n", (float)custom_time.count() / cudnn_time.count());
    
    // æ¸…ç†èµ„æº
    cudaFreeHost(h_input);
    cudaFreeHost(h_kernel);
    cudaFreeHost(h_output);
    cudaFree(d_input);
    cudaFree(d_kernel);
    cudaFree(d_output);
}
```

### 2. æ€§èƒ½åˆ†æç»“æœ
- **å°å·ç§¯æ ¸ (3Ã—3)**: cuDNNé€šå¸¸å¿«2-5å€
- **å¤§å·ç§¯æ ¸ (7Ã—7, 9Ã—9)**: cuDNNé€šå¸¸å¿«5-10å€
- **é•¿åºåˆ—**: cuDNNä¼˜åŠ¿æ›´æ˜æ˜¾
- **å†…å­˜å¸¦å®½**: cuDNNå†…å­˜è®¿é—®æ›´ä¼˜åŒ–

## å†…å­˜è®¿é—®ä¼˜åŒ–

### 1. æ•°æ®å¸ƒå±€ä¼˜åŒ–
```cpp
// ä¼˜åŒ–å‰ï¼šCHWæ ¼å¼
input[c * H * W + h * W + w]

// ä¼˜åŒ–åï¼šHWCæ ¼å¼
input[h * W * C + w * C + c]
```

### 2. å†…å­˜åˆå¹¶è®¿é—®
- ç›¸é‚»çº¿ç¨‹è®¿é—®ç›¸é‚»å†…å­˜åœ°å€
- ä½¿ç”¨å‘é‡åŒ–åŠ è½½/å­˜å‚¨æŒ‡ä»¤
- è€ƒè™‘å†…å­˜å¯¹é½

### 3. å…±äº«å†…å­˜ä½¿ç”¨ç­–ç•¥
- åˆç†é€‰æ‹©tileå¤§å°
- é¿å…bankå†²çª
- å¹³è¡¡å…±äº«å†…å­˜ä½¿ç”¨å’Œçº¿ç¨‹æ•°

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. çº¿ç¨‹å—å¤§å°ä¼˜åŒ–
```cpp
// è€ƒè™‘å› ç´ 
int threadsPerBlock = 256;  // æ€»çº¿ç¨‹æ•°
int tileH = 16, tileW = 16;  // tileå°ºå¯¸
int threadsPerTile = tileH * tileW;  // æ¯ä¸ªtileçš„çº¿ç¨‹æ•°
```

### 2. å¾ªç¯å±•å¼€
```cpp
// æ‰‹åŠ¨å±•å¼€å¾ªç¯
for (int kh = 0; kh < Kh; kh += 4) {
    sum += input[...] * kernel[kh] +
           input[...] * kernel[kh+1] +
           input[...] * kernel[kh+2] +
           input[...] * kernel[kh+3];
}
```

### 3. ä½¿ç”¨çº¹ç†å†…å­˜
```cpp
// å¯¹äºå…·æœ‰ç©ºé—´å±€éƒ¨æ€§çš„æ•°æ®
texture<float, 2, cudaReadModeElementType> texInput;
// åœ¨kernelä¸­ä½¿ç”¨tex2D(texInput, x, y)
```

## CNNå‰å‘ä¼ æ’­å®ç°

### 1. ç½‘ç»œç»“æ„
```cpp
struct ConvLayer {
    int inputH, inputW, inputC;
    int outputH, outputW, outputC;
    int kernelH, kernelW;
    int stride, padding;
    float *weights, *bias;
};
```

### 2. å‰å‘ä¼ æ’­
```cpp
void forwardPass(float *input, float *output, ConvLayer *layer) {
    // é…ç½®kernelå‚æ•°
    dim3 blockDim(16, 16);
    dim3 gridDim((layer->outputW + 15) / 16, 
                  (layer->outputH + 15) / 16);
    
    // å¯åŠ¨å·ç§¯kernel
    conv2dKernel<<<gridDim, blockDim>>>(
        input, layer->weights, output,
        layer->inputH, layer->inputW, layer->inputC,
        layer->kernelH, layer->kernelW, layer->outputC,
        layer->stride, layer->padding
    );
    
    // æ·»åŠ åç½®é¡¹
    addBiasKernel<<<gridDim, blockDim>>>(
        output, layer->bias, layer->outputH, layer->outputW, layer->outputC
    );
}
```

## ç¼–è¯‘å’Œè¿è¡Œ

### ç¼–è¯‘å‘½ä»¤
```bash
# åŸºç¡€ç‰ˆæœ¬
nvcc -O3 -arch=sm_70 -o cnn_conv cnn_conv.cu

# é“¾æ¥cuDNN
nvcc -O3 -arch=sm_70 -lcudnn -o cnn_conv_cudnn cnn_conv.cu

# é“¾æ¥cuBLAS (ç”¨äºçŸ©é˜µä¹˜æ³•)
nvcc -O3 -arch=sm_70 -lcudnn -lcublas -o cnn_conv_full cnn_conv.cu
```

### è¿è¡Œå‘½ä»¤
```bash
./cnn_conv
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•çŸ©é˜µå¤§å°
- è¾“å…¥å°ºå¯¸: 224Ã—224Ã—3 (ImageNetæ ‡å‡†)
- å·ç§¯æ ¸: 3Ã—3, 5Ã—5, 7Ã—7
- è¾“å‡ºé€šé“: 64, 128, 256

### æ€§èƒ½æŒ‡æ ‡
- FLOPS (æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°)
- å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
- è®¡ç®—æ•ˆç‡

## å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. å…±äº«å†…å­˜ä¸è¶³
- å‡å°‘tileå¤§å°
- ä½¿ç”¨åŠ¨æ€å…±äº«å†…å­˜
- é‡æ–°è®¾è®¡ç®—æ³•

### 2. è¾¹ç•Œå¤„ç†
- ä½¿ç”¨å¡«å……å€¼
- æ¡ä»¶åˆ¤æ–­ä¼˜åŒ–
- è€ƒè™‘ä½¿ç”¨æ¨¡æ¿å…ƒç¼–ç¨‹

### 3. ç²¾åº¦é—®é¢˜
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- è€ƒè™‘æ•°å€¼ç¨³å®šæ€§
- éªŒè¯è®¡ç®—ç»“æœ

## ä¸‹ä¸€æ­¥
æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶(Attention)å’ŒTransformerçš„å®ç°ï¼Œè¿™æ˜¯ç°ä»£NLPçš„åŸºç¡€ã€‚

## ç»ƒä¹ 
1. å®ç°ä¸åŒå·ç§¯æ ¸å¤§å°çš„ç‰ˆæœ¬ï¼Œæ¯”è¾ƒæ€§èƒ½
2. æ·»åŠ æ‰¹å¤„ç†æ”¯æŒï¼Œå¤„ç†å¤šä¸ªè¾“å…¥
3. å®ç°å·ç§¯å±‚çš„åå‘ä¼ æ’­
4. ä½¿ç”¨cuDNNåº“å¯¹æ¯”æ€§èƒ½
5. å®ç°Winogradå·ç§¯ç®—æ³•ä¼˜åŒ–

## å‚è€ƒèµ„æ–™
- [CUDA Convolution Implementation](https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/)
- [cuDNN Library](https://docs.nvidia.com/deeplearning/cudnn/)
- [CNN Architecture Design](https://arxiv.org/abs/1512.03385)
- [NVIDIA cuDNN Documentation](https://docs.nvidia.com/deeplearning/cudnn/)
- [cuDNN Performance Guide](https://docs.nvidia.com/deeplearning/cudnn/performance-guide/)
- [Convolution Algorithms](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#cudnnConvolutionFwdAlgo_t)
- [Winograd Convolution](https://arxiv.org/abs/1509.09308)
- [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)
- [cuDNN Convolution Performance](https://developer.nvidia.com/cudnn)
- [GPU Memory Management](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
- [Shared Memory Optimization](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)
- [Convolution Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network)
- [ImageNet Dataset](https://image-net.org/)
- [ResNet Architecture](https://arxiv.org/abs/1512.03385)
- [VGG Architecture](https://arxiv.org/abs/1409.1556)
- [AlexNet Architecture](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)

---

## ğŸ“ ç›¸å…³æ–‡ä»¶å¿«é€Ÿé“¾æ¥
æœ¬æ•™ç¨‹åŒ…å«ä»¥ä¸‹ç›¸å…³ç¨‹åºæ–‡ä»¶ï¼Œç‚¹å‡»å³å¯æŸ¥çœ‹ï¼š

### ğŸš€ ç¤ºä¾‹ç¨‹åº
- [`cnn_conv.cu`](cnn_conv.cu) - åŸºç¡€2Då·ç§¯å®ç°
- [`cnn_conv_optimized.cu`](cnn_conv_optimized.cu) - ä¼˜åŒ–ç‰ˆæœ¬å·ç§¯
- [`cnn_conv_cudnn.cu`](cnn_conv_cudnn.cu) - cuDNNç‰ˆæœ¬å·ç§¯
- [`cnn_forward.cu`](cnn_forward.cu) - CNNå‰å‘ä¼ æ’­å®ç°

### ğŸ“Š æ€§èƒ½åˆ†æå·¥å…·
- ä½¿ç”¨`nvprof`è¿›è¡Œå‘½ä»¤è¡Œæ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Systemsè¿›è¡Œç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Computeè¿›è¡Œkernelçº§æ€§èƒ½åˆ†æ

### ğŸ”§ ä¼˜åŒ–æŠ€å·§
- å…±äº«å†…å­˜tileä¼˜åŒ–
- å†…å­˜åˆå¹¶è®¿é—®ä¼˜åŒ–
- Winogradå·ç§¯ç®—æ³•
- æ··åˆç²¾åº¦è®¡ç®—
