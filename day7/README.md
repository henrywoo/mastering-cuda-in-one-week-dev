# Day 7: CUDAæ€§èƒ½è°ƒä¼˜é«˜çº§æŠ€å·§ - ä»ç†è®ºåˆ°å®è·µ

## æ¦‚è¿°
ä»Šå¤©æ˜¯æˆ‘ä»¬CUDAç¼–ç¨‹æ•™ç¨‹çš„æœ€åä¸€å¤©ï¼Œæˆ‘ä»¬å°†å­¦ä¹ CUDAæ€§èƒ½è°ƒä¼˜çš„é«˜çº§æŠ€å·§ã€‚è¿™äº›æŠ€å·§å°†å¸®åŠ©ä½ å°†CUDAç¨‹åºçš„æ€§èƒ½æå‡åˆ°æè‡´ï¼ŒåŒ…æ‹¬å†…å­˜ä¼˜åŒ–ã€æŒ‡ä»¤ä¼˜åŒ–ã€æ¶æ„ç‰¹å®šçš„ä¼˜åŒ–ä»¥åŠæ€§èƒ½åˆ†æå·¥å…·çš„ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜å°†æ·±å…¥äº†è§£ä¸åŒGPUæ¶æ„çš„ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬æœ€æ–°çš„Blackwellæ¶æ„å’Œå…¶ç‰¹æœ‰çš„ä¼˜åŒ–æŠ€æœ¯ã€‚

## å­¦ä¹ ç›®æ ‡
- æŒæ¡CUDAæ€§èƒ½è°ƒä¼˜çš„ç³»ç»Ÿæ–¹æ³•
- å­¦ä¼šä½¿ç”¨å„ç§æ€§èƒ½åˆ†æå·¥å…·
- ç†è§£å†…å­˜å±‚æ¬¡ç»“æ„çš„ä¼˜åŒ–ç­–ç•¥
- æŒæ¡æŒ‡ä»¤çº§å’Œçº¿ç¨‹çº§ä¼˜åŒ–æŠ€å·§
- å­¦ä¼šé’ˆå¯¹ç‰¹å®šGPUæ¶æ„è¿›è¡Œä¼˜åŒ–
- äº†è§£ä¸åŒGPUæ¶æ„çš„ç‰¹æ€§å’Œä¼˜åŒ–ç­–ç•¥

## GPUæ¶æ„æ¼”è¿›å’Œç‰¹æ€§

### 1. GPUæ¶æ„å‘å±•å†ç¨‹
```
Tesla (2006) â†’ Fermi (2010) â†’ Kepler (2012) â†’ Maxwell (2014) 
    â†“
Pascal (2016) â†’ Volta (2017) â†’ Turing (2018) â†’ Ampere (2020)
    â†“
Hopper (2022) â†’ Ada Lovelace (2022) â†’ Blackwell (2024)
```

### 2. å…³é”®æ¶æ„ç‰¹æ€§å¯¹æ¯”

#### Ampere (RTX 30ç³»åˆ—, A100)
- **è®¡ç®—èƒ½åŠ›**: 8.0, 8.6
- **Tensor Core**: ç¬¬ä¸‰ä»£ï¼Œæ”¯æŒFP16/BF16
- **RT Core**: ç¬¬äºŒä»£å…‰çº¿è¿½è¸ª
- **å†…å­˜**: GDDR6X, HBM2e
- **ç‰¹è‰²**: åŠ¨æ€å¹¶è¡Œã€å¤šå®ä¾‹GPU

#### Hopper (H100, H200)
- **è®¡ç®—èƒ½åŠ›**: 9.0
- **Tensor Core**: ç¬¬å››ä»£ï¼Œæ”¯æŒFP8
- **Transformer Engine**: ä¸“ç”¨AIåŠ é€Ÿ
- **å†…å­˜**: HBM3, 3TB/så¸¦å®½
- **ç‰¹è‰²**: åŠ¨æ€ç¼–ç¨‹ã€åä½œç»„

#### Ada Lovelace (RTX 40ç³»åˆ—)
- **è®¡ç®—èƒ½åŠ›**: 8.9
- **Tensor Core**: ç¬¬å››ä»£
- **RT Core**: ç¬¬ä¸‰ä»£
- **å†…å­˜**: GDDR6X, GDDR7
- **ç‰¹è‰²**: DLSS 3.0, AV1ç¼–ç 

#### Blackwell (B100, B200)
- **è®¡ç®—èƒ½åŠ›**: 9.0+
- **Tensor Core**: ç¬¬äº”ä»£ï¼Œæ”¯æŒtcgen05.mma
- **å†…å­˜**: HBM3e, 5TB/så¸¦å®½
- **ç‰¹è‰²**: æ–°ä¸€ä»£AIåŠ é€Ÿå¼•æ“

### 3. Blackwellæ¶æ„æ–°ç‰¹æ€§

#### tcgen05.mmaæŒ‡ä»¤
Blackwellå¼•å…¥äº†æ–°çš„Tensor CoreæŒ‡ä»¤ï¼Œå¦‚tcgen05.mmaï¼Œæä¾›æ›´é«˜çš„æ€§èƒ½å’Œçµæ´»æ€§ï¼š

```cpp
// Blackwell Tensor Coreä¼˜åŒ–ç¤ºä¾‹
__global__ void blackwellTensorCoreKernel(half *A, half *B, float *C,
                                         int M, int N, int K) {
    // ä½¿ç”¨æ–°çš„Tensor CoreæŒ‡ä»¤
    // æ³¨æ„ï¼šè¿™æ˜¯æ¦‚å¿µæ€§ä»£ç ï¼Œå®é™…æŒ‡ä»¤å¯èƒ½ä¸åŒ
    
    // åŠ è½½æ•°æ®åˆ°Tensor Core
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;
    
    // ä½¿ç”¨æ–°çš„tcgen05.mmaæŒ‡ä»¤
    // tcgen05.mma(a_frag, b_frag, c_frag);
    
    // å­˜å‚¨ç»“æœ
    wmma::store_matrix_sync(C, c_frag, 16, wmma::mem_row_major);
}
```

#### æ–°ä¸€ä»£AIåŠ é€Ÿå¼•æ“
- **æ›´é«˜çš„Tensor Coreå¯†åº¦**: æ¯ä¸ªSMæ›´å¤šçš„Tensor Core
- **æ”¹è¿›çš„å†…å­˜å±‚æ¬¡**: æ›´å¤§çš„L2ç¼“å­˜å’Œå…±äº«å†…å­˜
- **æ–°çš„æ•°æ®ç±»å‹æ”¯æŒ**: æ”¯æŒæ›´å¤šç²¾åº¦å’Œæ ¼å¼

## æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®º

### 1. æ€§èƒ½è°ƒä¼˜çš„å±‚æ¬¡
```
åº”ç”¨å±‚ä¼˜åŒ– (Algorithm)
    â†“
å†…å­˜è®¿é—®ä¼˜åŒ– (Memory Access)
    â†“
æŒ‡ä»¤çº§ä¼˜åŒ– (Instruction Level)
    â†“
æ¶æ„ç‰¹å®šä¼˜åŒ– (Architecture Specific)
```

### 2. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
- **è®¡ç®—ç“¶é¢ˆ**: æŒ‡ä»¤ååé‡ä¸è¶³
- **å†…å­˜ç“¶é¢ˆ**: å†…å­˜å¸¦å®½ä¸è¶³
- **å»¶è¿Ÿç“¶é¢ˆ**: å†…å­˜è®¿é—®å»¶è¿Ÿè¿‡é«˜
- **åŒæ­¥ç“¶é¢ˆ**: çº¿ç¨‹é—´åŒæ­¥å¼€é”€

## æ¶æ„ç‰¹å®šä¼˜åŒ–ç­–ç•¥

### 1. Ampereæ¶æ„ä¼˜åŒ–
```cpp
// Ampereç‰¹æœ‰çš„ä¼˜åŒ–
__global__ void ampereOptimizedKernel(float *data, int n) {
    // ä½¿ç”¨åä½œç»„ä¼˜åŒ–
    auto block = cooperative_groups::this_thread_block();
    
    // åˆ©ç”¨æ›´å¤§çš„å…±äº«å†…å­˜
    __shared__ float shared_data[16384];  // 16KBå…±äº«å†…å­˜
    
    // ä½¿ç”¨å¼‚æ­¥å†…å­˜æ“ä½œ
    // ...
}
```

### 2. Hopperæ¶æ„ä¼˜åŒ–
```cpp
// Hopperç‰¹æœ‰çš„ä¼˜åŒ–
__global__ void hopperOptimizedKernel(float *data, int n) {
    // ä½¿ç”¨Transformer Engine
    // åˆ©ç”¨FP8ç²¾åº¦
    
    // ä½¿ç”¨åŠ¨æ€ç¼–ç¨‹ç‰¹æ€§
    // ...
}
```

### 3. Blackwellæ¶æ„ä¼˜åŒ–
```cpp
// Blackwellç‰¹æœ‰çš„ä¼˜åŒ–
__global__ void blackwellOptimizedKernel(half *data, int n) {
    // ä½¿ç”¨æ–°çš„Tensor CoreæŒ‡ä»¤
    // åˆ©ç”¨æ›´å¤§çš„å†…å­˜å¸¦å®½
    
    // ä½¿ç”¨æ–°ä¸€ä»£AIåŠ é€Ÿå¼•æ“
    // ...
}
```

## å†…å­˜å±‚æ¬¡ç»“æ„ä¼˜åŒ–

### 1. å†…å­˜å±‚æ¬¡ç»“æ„
```
å¯„å­˜å™¨ (Register) - æœ€å¿«ï¼Œå®¹é‡æœ€å°
    â†“
å…±äº«å†…å­˜ (Shared Memory) - å¾ˆå¿«ï¼Œå®¹é‡æœ‰é™
    â†“
L2ç¼“å­˜ (L2 Cache) - è¾ƒå¿«ï¼Œå®¹é‡ä¸­ç­‰
    â†“
å…¨å±€å†…å­˜ (Global Memory) - è¾ƒæ…¢ï¼Œå®¹é‡æœ€å¤§
```

### 2. å¯„å­˜å™¨ä¼˜åŒ–
```cpp
// ä¼˜åŒ–å‰ï¼šé¢‘ç¹çš„å…¨å±€å†…å­˜è®¿é—®
__global__ void kernel1(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float sum = 0.0f;
        for (int i = 0; i < 100; i++) {
            sum += data[idx + i];  // æ¯æ¬¡å¾ªç¯éƒ½è®¿é—®å…¨å±€å†…å­˜
        }
        data[idx] = sum;
    }
}

// ä¼˜åŒ–åï¼šä½¿ç”¨å¯„å­˜å™¨ç¼“å­˜
__global__ void kernel2(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float sum = 0.0f;
        float temp[10];  // ä½¿ç”¨å¯„å­˜å™¨æ•°ç»„
        
        // æ‰¹é‡åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨
        for (int i = 0; i < 10; i++) {
            temp[i] = data[idx + i];
        }
        
        // åœ¨å¯„å­˜å™¨ä¸­è¿›è¡Œè®¡ç®—
        for (int i = 0; i < 10; i++) {
            sum += temp[i];
        }
        
        data[idx] = sum;
    }
}
```

### 3. å…±äº«å†…å­˜ä¼˜åŒ–
```cpp
// ä¼˜åŒ–å‰ï¼šç›´æ¥è®¿é—®å…¨å±€å†…å­˜
__global__ void matrixTransposeNaive(float *input, float *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x < width && y < height) {
        output[y * width + x] = input[x * height + y];
    }
}

// ä¼˜åŒ–åï¼šä½¿ç”¨å…±äº«å†…å­˜
__global__ void matrixTransposeShared(float *input, float *output, int width, int height) {
    __shared__ float tile[TILE_SIZE][TILE_SIZE];
    
    int x = blockIdx.x * TILE_SIZE + threadIdx.x;
    int y = blockIdx.y * TILE_SIZE + threadIdx.y;
    
    // åä½œåŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }
    __syncthreads();
    
    // åä½œå†™å…¥è¾“å‡º
    int newX = blockIdx.y * TILE_SIZE + threadIdx.x;
    int newY = blockIdx.x * TILE_SIZE + threadIdx.y;
    
    if (newX < height && newY < width) {
        output[newY * height + newX] = tile[threadIdx.x][threadIdx.y];
    }
}
```

### 4. å†…å­˜åˆå¹¶è®¿é—®ä¼˜åŒ–
```cpp
// ä¼˜åŒ–å‰ï¼šå†…å­˜è®¿é—®ä¸åˆå¹¶
__global__ void memoryCoalescingBad(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = 256;  // å¤§æ­¥é•¿ï¼Œå¯¼è‡´å†…å­˜è®¿é—®ä¸åˆå¹¶
    
    if (idx < n) {
        data[idx * stride] = idx;  // å†…å­˜è®¿é—®é—´éš”å¾ˆå¤§
    }
}

// ä¼˜åŒ–åï¼šå†…å­˜è®¿é—®åˆå¹¶
__global__ void memoryCoalescingGood(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        data[idx] = idx;  // è¿ç»­çš„å†…å­˜è®¿é—®
    }
}
```

## æŒ‡ä»¤çº§ä¼˜åŒ–

### 1. å¾ªç¯å±•å¼€
```cpp
// ä¼˜åŒ–å‰ï¼šæ ‡å‡†å¾ªç¯
__global__ void loopUnrollBad(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        for (int i = 0; i < 4; i++) {
            c[idx * 4 + i] = a[idx * 4 + i] + b[idx * 4 + i];
        }
    }
}

// ä¼˜åŒ–åï¼šæ‰‹åŠ¨å¾ªç¯å±•å¼€
__global__ void loopUnrollGood(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        c[idx * 4 + 0] = a[idx * 4 + 0] + b[idx * 4 + 0];
        c[idx * 4 + 1] = a[idx * 4 + 1] + b[idx * 4 + 1];
        c[idx * 4 + 2] = a[idx * 4 + 2] + b[idx * 4 + 2];
        c[idx * 4 + 3] = a[idx * 4 + 3] + b[idx * 4 + 3];
    }
}
```

### 2. åˆ†æ”¯ä¼˜åŒ–
```cpp
// ä¼˜åŒ–å‰ï¼šå­˜åœ¨åˆ†æ”¯åˆ†æ­§
__global__ void branchDivergenceBad(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        if (data[idx] > 0) {
            data[idx] = sqrtf(data[idx]);  // éƒ¨åˆ†çº¿ç¨‹æ‰§è¡Œ
        } else {
            data[idx] = 0.0f;  // éƒ¨åˆ†çº¿ç¨‹æ‰§è¡Œ
        }
    }
}

// ä¼˜åŒ–åï¼šå‡å°‘åˆ†æ”¯åˆ†æ­§
__global__ void branchDivergenceGood(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        float val = data[idx];
        float result = (val > 0) ? sqrtf(val) : 0.0f;
        data[idx] = result;
    }
}
```

### 3. å‘é‡åŒ–æ“ä½œ
```cpp
// ä½¿ç”¨å‘é‡åŒ–æ•°æ®ç±»å‹
__global__ void vectorizedKernel(float4 *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        float4 val = data[idx];
        val.x *= 2.0f;
        val.y *= 2.0f;
        val.z *= 2.0f;
        val.w *= 2.0f;
        data[idx] = val;
    }
}
```

## çº¿ç¨‹çº§ä¼˜åŒ–

### 1. çº¿ç¨‹å—å¤§å°ä¼˜åŒ–
```cpp
// è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çº¿ç¨‹å—å¤§å°
int getOptimalBlockSize(int deviceId) {
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, deviceId);
    
    // è€ƒè™‘å…±äº«å†…å­˜é™åˆ¶
    int maxThreadsPerSM = prop.maxThreadsPerMultiProcessor;
    int maxSharedMemoryPerSM = prop.sharedMemoryPerMultiprocessor;
    
    // è€ƒè™‘å¯„å­˜å™¨é™åˆ¶
    int maxRegistersPerSM = prop.regsPerMultiprocessor;
    
    // è¿”å›æœ€ä¼˜é…ç½®
    return min(256, maxThreadsPerSM / prop.multiProcessorCount);
}
```

### 2. ç½‘æ ¼å¤§å°ä¼˜åŒ–
```cpp
// è®¡ç®—æœ€ä¼˜ç½‘æ ¼å¤§å°
dim3 getOptimalGridSize(int n, int blockSize) {
    int blocksX = (n + blockSize - 1) / blockSize;
    int blocksY = 1;
    int blocksZ = 1;
    
    // è€ƒè™‘GPUçš„SMæ•°é‡
    cudaDeviceProp prop;
    cudaGetDevice(&prop);
    int maxBlocksPerSM = prop.maxBlocksPerMultiProcessor;
    int totalBlocks = blocksX * blocksY * blocksZ;
    
    // ç¡®ä¿æœ‰è¶³å¤Ÿçš„SMæ¥å¹¶è¡Œæ‰§è¡Œ
    if (totalBlocks < prop.multiProcessorCount * maxBlocksPerSM) {
        // å¯ä»¥å¢åŠ ç½‘æ ¼å¤§å°
        blocksX = max(blocksX, prop.multiProcessorCount);
    }
    
    return dim3(blocksX, blocksY, blocksZ);
}
```

## æ¶æ„ç‰¹å®šä¼˜åŒ–

### 1. Tensor Coreä¼˜åŒ– (Volta+)
```cpp
// ä½¿ç”¨Tensor Coreè¿›è¡ŒçŸ©é˜µä¹˜æ³•
__global__ void tensorCoreMatMul(half *A, half *B, float *C,
                                int M, int N, int K) {
    // ä½¿ç”¨wmma API
    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;
    
    // åŠ è½½æ•°æ®
    wmma::load_matrix_sync(a_frag, A, 16);
    wmma::load_matrix_sync(b_frag, B, 16);
    
    // æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
    wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    
    // å­˜å‚¨ç»“æœ
    wmma::store_matrix_sync(C, c_frag, 16, wmma::mem_row_major);
}
```

### 2. å…±äº«å†…å­˜Bankä¼˜åŒ–
```cpp
// é¿å…Bankå†²çª
__global__ void bankConflictFree(float *input, float *output, int n) {
    __shared__ float shared_data[1024];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        // ä½¿ç”¨äº¤é”™ç´¢å¼•é¿å…Bankå†²çª
        int shared_idx = (tid * 33) % 1024;  // 33æ˜¯è´¨æ•°ï¼Œé¿å…Bankå†²çª
        shared_data[shared_idx] = input[idx];
        __syncthreads();
        
        output[idx] = shared_data[shared_idx];
    }
}
```

## æ€§èƒ½åˆ†æå·¥å…·

### 1. Nsight Systems
```bash
# å‘½ä»¤è¡Œä½¿ç”¨
nsys profile --stats=true ./your_program

# ç”ŸæˆæŠ¥å‘Š
nsys export --type sqlite --output report.sqlite profile.qdrep
```

### 2. Nsight Compute
```bash
# åˆ†æç‰¹å®škernel
ncu --set full --kernel-regex ".*" ./your_program

# å¯¼å‡ºæŒ‡æ ‡
ncu --csv --log-file metrics.csv ./your_program
```

### 3. è‡ªå®šä¹‰æ€§èƒ½è®¡æ•°å™¨
```cpp
// ä½¿ç”¨CUDAäº‹ä»¶æµ‹é‡æ—¶é—´
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start);
// æ‰§è¡Œkernel
cudaEventRecord(stop);

cudaEventSynchronize(stop);
float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop);

printf("Kernel execution time: %f ms\n", milliseconds);
```

## æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ

### 1. ç³»ç»ŸåŒ–æ–¹æ³•
1. **åŸºå‡†æµ‹è¯•**: å»ºç«‹æ€§èƒ½åŸºçº¿
2. **ç“¶é¢ˆè¯†åˆ«**: ä½¿ç”¨åˆ†æå·¥å…·æ‰¾å‡ºç“¶é¢ˆ
3. **ä¼˜åŒ–å®æ–½**: åº”ç”¨ç›¸åº”çš„ä¼˜åŒ–æŠ€æœ¯
4. **æ€§èƒ½éªŒè¯**: éªŒè¯ä¼˜åŒ–æ•ˆæœ
5. **è¿­ä»£ä¼˜åŒ–**: é‡å¤ä¸Šè¿°è¿‡ç¨‹

### 2. å¸¸è§ä¼˜åŒ–æ¨¡å¼
```cpp
// æ¨¡å¼1ï¼šå†…å­˜è®¿é—®ä¼˜åŒ–
__global__ void memoryOptimizedKernel(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜
    __shared__ float cache[256];
    if (idx < n) {
        cache[threadIdx.x] = data[idx];
        __syncthreads();
        
        // åœ¨å…±äº«å†…å­˜ä¸­è¿›è¡Œè®¡ç®—
        // ...
    }
}

// æ¨¡å¼2ï¼šè®¡ç®—ä¼˜åŒ–
__global__ void computeOptimizedKernel(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        // ä½¿ç”¨æ•°å­¦å‡½æ•°è¿‘ä¼¼
        float x = data[idx];
        float result = __fdividef(x, 2.0f);  // å¿«é€Ÿé™¤æ³•
        data[idx] = result;
    }
}
```

## ç¼–è¯‘ä¼˜åŒ–

### 1. ç¼–è¯‘å™¨æ ‡å¿—
```bash
# ä¼˜åŒ–æ ‡å¿—
nvcc -O3 -Xptxas -O3,-v your_program.cu

# ç‰¹å®šæ¶æ„ä¼˜åŒ–
nvcc -code=sm_80 your_program.cu

# è°ƒè¯•ä¿¡æ¯
nvcc -g -G your_program.cu

# Blackwellæ¶æ„ä¼˜åŒ–
nvcc -o blackwell_optimized your_program.cu
```

### 2. å†…è”ä¼˜åŒ–
```cpp
// ä½¿ç”¨__forceinline__å¼ºåˆ¶å†…è”
__forceinline__ __device__ float fastSqrt(float x) {
    return __fsqrt_rn(x);
}
```

## æ€§èƒ½è°ƒä¼˜æ¡ˆä¾‹ç ”ç©¶

### 1. çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
```cpp
// ä»åŸºç¡€å®ç°åˆ°ä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½æå‡
// åŸºç¡€ç‰ˆæœ¬: ~100 GFLOPS
// å…±äº«å†…å­˜ç‰ˆæœ¬: ~500 GFLOPS
// å¯„å­˜å™¨ä¼˜åŒ–ç‰ˆæœ¬: ~800 GFLOPS
// Tensor Coreç‰ˆæœ¬: ~2000+ GFLOPS
// Blackwellä¼˜åŒ–ç‰ˆæœ¬: ~3000+ GFLOPS (é¢„æœŸ)
```

### 2. å·ç§¯ä¼˜åŒ–
```cpp
// ä¸åŒä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½å¯¹æ¯”
// åŸºç¡€ç‰ˆæœ¬: å†…å­˜å¸¦å®½å—é™
// å…±äº«å†…å­˜ç‰ˆæœ¬: å†…å­˜è®¿é—®ä¼˜åŒ–
// åˆ†ç¦»å·ç§¯ç‰ˆæœ¬: è®¡ç®—å¤æ‚åº¦é™ä½
// Winogradç‰ˆæœ¬: ç®—æ³•çº§ä¼˜åŒ–
// cuDNNç‰ˆæœ¬: åº“çº§ä¼˜åŒ–
```

## ç¼–è¯‘å’Œè¿è¡Œ

### ç¼–è¯‘å‘½ä»¤
```bash
# åŸºç¡€ç‰ˆæœ¬
nvcc -O3 -o performance_tuning performance_tuning.cu

# é’ˆå¯¹ç‰¹å®šæ¶æ„ä¼˜åŒ–
nvcc -O3 -o blackwell_tuning performance_tuning.cu

# å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
nvcc -O3 -Xptxas -O3,-v -lcublas -o full_optimized performance_tuning.cu
```

### è¿è¡Œå‘½ä»¤
```bash
./performance_tuning
```

## æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

### 1. å†…å­˜ä¼˜åŒ–
- [ ] ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
- [ ] ç¡®ä¿å†…å­˜åˆå¹¶è®¿é—®
- [ ] ä¼˜åŒ–å†…å­˜å¸ƒå±€å’Œæ•°æ®ç»“æ„
- [ ] ä½¿ç”¨å†…å­˜æ± å‡å°‘åˆ†é…å¼€é”€

### 2. è®¡ç®—ä¼˜åŒ–
- [ ] å‡å°‘åˆ†æ”¯åˆ†æ­§
- [ ] ä½¿ç”¨å¾ªç¯å±•å¼€
- [ ] åˆ©ç”¨å‘é‡åŒ–æŒ‡ä»¤
- [ ] é€‰æ‹©æœ€ä¼˜çš„æ•°å­¦å‡½æ•°

### 3. çº¿ç¨‹ä¼˜åŒ–
- [ ] é€‰æ‹©æœ€ä¼˜çº¿ç¨‹å—å¤§å°
- [ ] ä¼˜åŒ–ç½‘æ ¼é…ç½®
- [ ] å‡å°‘åŒæ­¥å¼€é”€
- [ ] å¹³è¡¡è´Ÿè½½åˆ†å¸ƒ

### 4. æ¶æ„ä¼˜åŒ–
- [ ] ä½¿ç”¨Tensor Core (å¦‚æœå¯ç”¨)
- [ ] ä¼˜åŒ–å…±äº«å†…å­˜Bankä½¿ç”¨
- [ ] åˆ©ç”¨L2ç¼“å­˜
- [ ] è€ƒè™‘å¯„å­˜å™¨ä½¿ç”¨
- [ ] é’ˆå¯¹ç‰¹å®šGPUæ¶æ„ä¼˜åŒ–

## æ€»ç»“

ç»è¿‡è¿™7å¤©çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†ï¼š

1. **Day 1**: CUDAåŸºç¡€ - å‘é‡åŠ æ³•
2. **Day 2**: æ·±å…¥åº•å±‚ - PTXä»£ç åŠ è½½
3. **Day 3**: çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
4. **Day 4**: CNNå·ç§¯å®ç°
5. **Day 5**: æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer
6. **Day 6**: æœ€æ–°LLM CUDA kernelå®šåˆ¶ä¼˜åŒ–
7. **Day 7**: æ€§èƒ½è°ƒä¼˜é«˜çº§æŠ€å·§

è¿™äº›çŸ¥è¯†æ„æˆäº†CUDAç¼–ç¨‹çš„å®Œæ•´ä½“ç³»ï¼Œä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§ä¼˜åŒ–ï¼Œä»ç®€å•ç®—æ³•åˆ°å¤æ‚æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å¸Œæœ›è¿™ä¸ªæ•™ç¨‹èƒ½å¤Ÿå¸®åŠ©ä½ æˆä¸ºCUDAç¼–ç¨‹çš„é«˜æ‰‹ï¼

## ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

1. **æ·±å…¥å­¦ä¹ **: ç ”ç©¶cuDNNã€cuBLASç­‰åº“çš„å®ç°
2. **å®é™…é¡¹ç›®**: å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­
3. **æ€§èƒ½è°ƒä¼˜**: æŒç»­å­¦ä¹ å’Œå®è·µæ€§èƒ½ä¼˜åŒ–æŠ€å·§
4. **æ–°ç‰¹æ€§**: å…³æ³¨CUDAæ–°ç‰ˆæœ¬çš„æ–°ç‰¹æ€§
5. **æ¶æ„ç ”ç©¶**: æ·±å…¥ç ”ç©¶ä¸åŒGPUæ¶æ„çš„ä¼˜åŒ–ç­–ç•¥

## å‚è€ƒèµ„æ–™
- [CUDA Performance Optimization](https://developer.nvidia.com/cuda-zone)
- [Nsight Tools Documentation](https://developer.nvidia.com/nsight-graphics)
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [GPU Performance Analysis](https://developer.nvidia.com/blog/analyzing-gpu-performance/)
- [NVIDIA Blackwell Architecture](https://www.nvidia.com/en-us/data-center/blackwell/)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Tensor Core Programming](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/)
- [GPU Architecture Comparison](https://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units)
- [Ampere Architecture](https://www.nvidia.com/en-us/data-center/ampere-gpu-architecture/)
- [Hopper Architecture](https://www.nvidia.com/en-us/data-center/hopper-gpu-architecture/)
- [Ada Lovelace Architecture](https://www.nvidia.com/en-us/geforce/ada/)
- [CUDA Performance Optimization](https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/)
- [GPU Memory Management](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)
- [Shared Memory Optimization](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)
- [Branch Divergence Optimization](https://developer.nvidia.com/blog/cuda-pro-tip-optimize-using-vectorized-memory-access/)
- [Loop Unrolling Techniques](https://developer.nvidia.com/blog/cuda-pro-tip-optimize-using-vectorized-memory-access/)
- [Tensor Core Programming](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/)
- [CUDA Compiler Optimization](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/)
- [PTX Assembly Reference](https://docs.nvidia.com/cuda/parallel-thread-execution/)
- [GPU Performance Counters](https://docs.nvidia.com/cuda/profiler-guide/)
- [NVIDIA Developer Blog](https://developer.nvidia.com/blog/)

---

## ğŸ“ ç›¸å…³æ–‡ä»¶å¿«é€Ÿé“¾æ¥
æœ¬æ•™ç¨‹åŒ…å«ä»¥ä¸‹ç›¸å…³ç¨‹åºæ–‡ä»¶ï¼Œç‚¹å‡»å³å¯æŸ¥çœ‹ï¼š

### ğŸš€ ç¤ºä¾‹ç¨‹åº
- [`performance_tuning.cu`](performance_tuning.cu) - æ€§èƒ½è°ƒä¼˜ç¤ºä¾‹ç¨‹åº
- [`blackwell_tuning.cu`](blackwell_tuning.cu) - Blackwellæ¶æ„ä¼˜åŒ–ç¤ºä¾‹
- [`memory_optimization.cu`](memory_optimization.cu) - å†…å­˜ä¼˜åŒ–ç¤ºä¾‹
- [`instruction_optimization.cu`](instruction_optimization.cu) - æŒ‡ä»¤ä¼˜åŒ–ç¤ºä¾‹

### ğŸ“Š æ€§èƒ½åˆ†æå·¥å…·
- ä½¿ç”¨`nvprof`è¿›è¡Œå‘½ä»¤è¡Œæ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Systemsè¿›è¡Œç³»ç»Ÿçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨Nsight Computeè¿›è¡Œkernelçº§æ€§èƒ½åˆ†æ
- ä½¿ç”¨GPUæ€§èƒ½è®¡æ•°å™¨è¿›è¡Œè¯¦ç»†åˆ†æ

### ğŸ”§ ä¼˜åŒ–æŠ€å·§
- å†…å­˜å±‚æ¬¡ç»“æ„ä¼˜åŒ–
- æŒ‡ä»¤çº§å¹¶è¡Œä¼˜åŒ–
- çº¿ç¨‹çº§å¹¶è¡Œä¼˜åŒ–
- æ¶æ„ç‰¹å®šä¼˜åŒ–
- Tensor CoreæŒ‡ä»¤ä¼˜åŒ–
